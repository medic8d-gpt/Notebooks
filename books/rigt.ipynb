{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 561384,
     "status": "ok",
     "timestamp": 1762584101255,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     },
     "user_tz": 300
    },
    "id": "3TUA4qWYFGqr",
    "outputId": "0bb5e6c4-8678-4a47-e829-71f92e1e4527",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Step 1: Mount Google Drive ---\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive mounted successfully.\n",
      "\n",
      "--- Running Step 2: Configuring Paths ---\n",
      "Reading from: /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset\n",
      "Will write count to: /content/drive/MyDrive/KDN_Archive_Downloads/word_frequency.txt\n",
      "\n",
      "--- Running Step 3: Initializing Word Counter ---\n",
      "\n",
      "--- Running Step 4: Starting Word Count Process ---\n",
      "Starting scan of /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset...\n",
      "  ...scanned 500 files...\n",
      "  ...scanned 1000 files...\n",
      "  ...scanned 1500 files...\n",
      "  ...scanned 2000 files...\n",
      "Scan complete. Found 2163205 unique words in 2474 files.\n",
      "\n",
      "Writing word list to /content/drive/MyDrive/KDN_Archive_Downloads/word_frequency.txt...\n",
      "Successfully wrote 2163205 unique words.\n",
      "Total time: 560.85 seconds.\n",
      "\n",
      "--- All tasks finished ---\n",
      "You can now open '/content/drive/MyDrive/KDN_Archive_Downloads/word_frequency.txt' in your Google Drive,\n",
      "find the misspelled words, and add them to 'kdn_cleaner.py'.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# This script is a tool to help you build a better correction\n",
    "# dictionary for the 'kdn_cleaner.py' script.\n",
    "#\n",
    "# It does NOT clean any files.\n",
    "# It reads all the GARBLED files in your organized directory\n",
    "# and creates a new file called 'word_frequency.txt' that\n",
    "# lists every word and how many times it appears,\n",
    "# sorted from most frequent to least.\n",
    "#\n",
    "# You can then look at this 'word_frequency.txt' file,\n",
    "# easily spot the misspelled words (like 'fubfcriber'),\n",
    "# and add them to the OCR_ERROR_MAP in 'kdn_cleaner.py'.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from google.colab import drive\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 1: Mount your Google Drive\n",
    "# ------------------------------------------------------------------\n",
    "print(\"--- Running Step 1: Mount Google Drive ---\")\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting drive: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 2: Define Directories\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 2: Configuring Paths ---\")\n",
    "\n",
    "# This is the directory with your ORGANIZED but GARBLED files\n",
    "SOURCE_DIRECTORY = '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset'\n",
    "\n",
    "# This is the file where the word count will be saved\n",
    "OUTPUT_FILE = '/content/drive/MyDrive/KDN_Archive_Downloads/word_frequency.txt'\n",
    "\n",
    "print(f\"Reading from: {SOURCE_DIRECTORY}\")\n",
    "print(f\"Will write count to: {OUTPUT_FILE}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 3: Word Counting Function\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 3: Initializing Word Counter ---\")\n",
    "\n",
    "# A regex to find words: sequences of alphabetic characters\n",
    "# It will ignore numbers and simple punctuation.\n",
    "word_regex = re.compile(r'\\b[a-z]+\\b')\n",
    "\n",
    "def count_words(source_dir):\n",
    "    \"\"\"\n",
    "    Walks all subdirectories, reads all .txt files, counts all words.\n",
    "    \"\"\"\n",
    "    word_counter = Counter()\n",
    "    file_count = 0\n",
    "\n",
    "    print(f\"Starting scan of {source_dir}...\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        text_content = f.read()\n",
    "\n",
    "                    # Find all words, convert to lowercase, and update count\n",
    "                    words = word_regex.findall(text_content.lower())\n",
    "                    word_counter.update(words)\n",
    "                    file_count += 1\n",
    "\n",
    "                    if file_count % 500 == 0:\n",
    "                        print(f\"  ...scanned {file_count} files...\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  ERROR: Could not process file {filename}. {e}\")\n",
    "\n",
    "    print(f\"Scan complete. Found {len(word_counter)} unique words in {file_count} files.\")\n",
    "    return word_counter\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 4: Main Processing Loop\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 4: Starting Word Count Process ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "word_counts = count_words(SOURCE_DIRECTORY)\n",
    "\n",
    "print(f\"\\nWriting word list to {OUTPUT_FILE}...\")\n",
    "try:\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Word Frequency List\\n\")\n",
    "        f.write(\"---------------------\\n\")\n",
    "        f.write(f\"Source: {SOURCE_DIRECTORY}\\n\")\n",
    "        f.write(\"---------------------\\n\\n\")\n",
    "\n",
    "        # Sort words by frequency, from most common to least\n",
    "        for word, count in word_counts.most_common():\n",
    "            f.write(f\"{count: <10}{word}\\n\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Successfully wrote {len(word_counts)} unique words.\")\n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  ERROR: Failed to write output file. {e}\")\n",
    "\n",
    "print(\"\\n--- All tasks finished ---\")\n",
    "print(f\"You can now open '{OUTPUT_FILE}' in your Google Drive,\")\n",
    "print(\"find the misspelled words, and add them to 'kdn_cleaner.py'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BXv6shT6YYdC",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import requests # Import the requests library for making API calls\n",
    "\n",
    "# Define the output directory and glob pattern as used previously\n",
    "output_directory = '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset'\n",
    "glob_pattern = \"*_djvu.txt\"\n",
    "\n",
    "# Add explicit checks and print statements for the output directory\n",
    "print(f\"Checking directory: {output_directory}\")\n",
    "if os.path.exists(output_directory):\n",
    "    print(\"Output directory exists.\")\n",
    "    # Optional: Add a small delay to allow file system to sync\n",
    "    # time.sleep(2)\n",
    "\n",
    "    # Get a list of downloaded files to identify the last one\n",
    "    try:\n",
    "        downloaded_files = []\n",
    "        # List all entries in the main download directory (expected to be item ID directories)\n",
    "        all_entries = os.listdir(output_directory)\n",
    "        print(f\"Entries found in main directory (expected item IDs): {len(all_entries)}\")\n",
    "\n",
    "        for entry in all_entries:\n",
    "            item_dir = os.path.join(output_directory, entry)\n",
    "            # Check if the entry is a directory\n",
    "            if os.path.isdir(item_dir):\n",
    "                # List files within the item ID directory\n",
    "                files_in_item_dir = os.listdir(item_dir)\n",
    "                # Filter for the expected glob pattern\n",
    "                for file_name in files_in_item_dir:\n",
    "                    if file_name.endswith('_djvu.txt'): # More specific check than just .txt\n",
    "                        downloaded_files.append(os.path.join(item_dir, file_name))\n",
    "\n",
    "        # Sort the found files by modification time to find the last downloaded\n",
    "        downloaded_files.sort(key=os.path.getmtime)\n",
    "\n",
    "        print(f\"Filtered downloaded files (.txt): {len(downloaded_files)}\")\n",
    "        if downloaded_files:\n",
    "            last_downloaded_file = downloaded_files[-1]\n",
    "            # Extract the item ID from the last downloaded file name\n",
    "            last_downloaded_item_id = os.path.basename(last_downloaded_file).split('_', 1)[0]\n",
    "            print(f\"Last downloaded file identified: {last_downloaded_file}\")\n",
    "            print(f\"Last downloaded item ID: {last_downloaded_item_id}\")\n",
    "        else:\n",
    "            last_downloaded_item_id = None\n",
    "            print(\"No previous .txt downloads found in the directory or its subdirectories.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying last downloaded file: {e}\")\n",
    "        last_downloaded_item_id = None\n",
    "\n",
    "\n",
    "# Find the index of the last downloaded item in the full item list\n",
    "start_index = 0\n",
    "if last_downloaded_item_id and item_identifiers:\n",
    "    try:\n",
    "        start_index = item_identifiers.index(last_downloaded_item_id) + 1\n",
    "        print(f\"Resuming download from item index: {start_index} (after {last_downloaded_item_id})\")\n",
    "    except ValueError:\n",
    "        print(f\"Last downloaded item ID '{last_downloaded_item_id}' not found in the full item list. Starting from the beginning.\")\n",
    "        start_index = 0\n",
    "\n",
    "# Get the list of items remaining to download starting from the identified index\n",
    "remaining_item_ids_for_download = item_identifiers[start_index:]\n",
    "\n",
    "# Determine the total number of items to download in this session\n",
    "total_items_to_download_this_session = len(remaining_item_ids_for_download)\n",
    "print(f\"Starting download of {total_items_to_download_this_session} remaining items...\")\n",
    "\n",
    "# Define the batch size for processing\n",
    "batch_size = 10 # You can adjust this number\n",
    "\n",
    "# Initialize download counter to keep track of overall progress\n",
    "download_counter = start_index\n",
    "\n",
    "# Iterate through the remaining item IDs in batches\n",
    "for i in range(0, total_items_to_download_this_session, batch_size):\n",
    "    batch_item_ids = remaining_item_ids_for_download[i:i + batch_size]\n",
    "    current_batch_num = (i // batch_size) + 1\n",
    "    total_batches = (total_items_to_download_this_session + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"\\n--- Processing Batch {current_batch_num}/{total_batches} ({len(batch_item_ids)} items) ---\")\n",
    "\n",
    "    # Get and display newspaper/issue info for the first item in the batch\n",
    "    if batch_item_ids: # Ensure batch is not empty\n",
    "        first_item_id_in_batch = batch_item_ids[0]\n",
    "        metadata_url = f\"https://archive.org/metadata/{first_item_id_in_batch}\"\n",
    "        try:\n",
    "            response = requests.get(metadata_url)\n",
    "            response.raise_for_status()\n",
    "            metadata = response.json()\n",
    "\n",
    "            title = metadata.get('metadata', {}).get('title', 'N/A')\n",
    "            date = metadata.get('metadata', {}).get('date', 'N/A')\n",
    "\n",
    "            print(f\"  - Currently processing: Item ID: {first_item_id_in_batch}, Title: {title}, Date: {date}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  - Error fetching metadata for {first_item_id_in_batch}: {e}\")\n",
    "\n",
    "\n",
    "    # Prepare the batch item IDs for piping to standard input\n",
    "    batch_items_input = \"\\n\".join(batch_item_ids)\n",
    "\n",
    "    # Construct the ia download command for the current batch\n",
    "    download_command = [\n",
    "        'ia',\n",
    "        'download',\n",
    "        '--itemlist=-',\n",
    "        f'--destdir={output_directory}',\n",
    "        f'--glob={glob_pattern}',\n",
    "        '--retries', '5'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(\n",
    "            download_command,\n",
    "            input=batch_items_input,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            check=True,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        batch_duration = end_time - start_time\n",
    "\n",
    "        print(f\"Batch {current_batch_num} completed in {batch_duration:.2f} seconds.\")\n",
    "        # Optional: Print stdout/stderr if needed for debugging\n",
    "        # if result.stdout:\n",
    "        #     print(\"Batch Stdout:\")\n",
    "        #     print(result.stdout)\n",
    "        # if result.stderr:\n",
    "        #     print(\"Batch Stderr:\")\n",
    "        #     print(result.stderr)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 'ia' command not found during batch {current_batch_num}. Ensure internetarchive is installed and in your PATH.\")\n",
    "        break\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error executing ia download for batch {current_batch_num}: {e}\")\n",
    "        print(f\"Stdout: {e.stdout}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Error: ia download command timed out for batch {current_batch_num}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during batch {current_batch_num}: {e}\")\n",
    "\n",
    "    # Update download_counter based on the number of items processed in the batch\n",
    "    download_counter = start_index + i + len(batch_item_ids)\n",
    "\n",
    "\n",
    "print(\"\\n--- All Batches Processed ---\")\n",
    "print(f\"Check the folder '{output_directory}' in your Google Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1341,
     "status": "ok",
     "timestamp": 1762583217925,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     },
     "user_tz": 300
    },
    "id": "3gFFPpqjBxQD",
    "outputId": "0148abe3-6ddf-4a35-a74e-84b47ad3b5e9",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Step 1: Check/Install 'internetarchive' library ---\n",
      "internetarchive library is already installed.\n",
      "\n",
      "--- Running Step 2: Mount Google Drive ---\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive mounted successfully.\n",
      "List of unique newspaper titles will be saved to: /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Newspaper_Titles.txt\n",
      "\n",
      "--- Running Step 4: Get All Unique Titles (Python Method) ---\n",
      "Running search for: collection:kentuckynewspapers AND (place_of_publication:\"Lexington\" OR title:\"Lexington\")\n",
      "This will take a few minutes...\n",
      "Processed 500 of 2478 items...\n",
      "Processed 1000 of 2478 items...\n",
      "Processed 1500 of 2478 items...\n",
      "Processed 2000 of 2478 items...\n",
      "Search complete. Processed 2478 total items.\n",
      "\n",
      "--- Running Step 5: Save and Count Unique Titles ---\n",
      "Saving unique titles list to /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Newspaper_Titles.txt...\n",
      "Save complete.\n",
      "\n",
      "--- FINAL COUNT ---\n",
      "Total unique newspaper titles found: 26\n",
      "\n",
      "--- All tasks finished ---\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# This script is designed to be run in a single Google Colab cell.\n",
    "# It will ANALYZE the 2,478 items from the Lexington-specific\n",
    "# search query and create a list of all the unique newspaper\n",
    "# titles (not the individual issue titles) found in that search.\n",
    "# It does NOT download any files.\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 1: Install 'internetarchive' (if needed)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 1: Check/Install 'internetarchive' library ---\")\n",
    "try:\n",
    "    import internetarchive\n",
    "    print(\"internetarchive library is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"internetarchive library not found. Installing...\")\n",
    "    !pip install internetarchive\n",
    "    print(\"Installation complete.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 2: Mount your Google Drive\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 2: Mount Google Drive ---\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting drive: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 3: Set up File Paths\n",
    "# ------------------------------------------------------------------\n",
    "import os\n",
    "output_directory = '/content/drive/MyDrive/KDN_Archive_Downloads'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# This is the file where the final list will be saved.\n",
    "unique_titles_list = f\"{output_directory}/Lexington_Newspaper_Titles.txt\"\n",
    "\n",
    "print(f\"List of unique newspaper titles will be saved to: {unique_titles_list}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 4: Get All Unique Newspaper Titles from the Lexington Query\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 4: Get All Unique Titles (Python Method) ---\")\n",
    "import internetarchive\n",
    "import sys\n",
    "\n",
    "# This is the same query from your downloader script.\n",
    "subset_query = 'collection:kentuckynewspapers AND (place_of_publication:\"Lexington\" OR title:\"Lexington\")'\n",
    "print(f\"Running search for: {subset_query}\")\n",
    "print(\"This will take a few minutes...\")\n",
    "\n",
    "# A 'set' will automatically store only unique entries.\n",
    "unique_newspaper_titles = set()\n",
    "\n",
    "try:\n",
    "    # We only need the 'title' field from the search\n",
    "    search_results = internetarchive.search_items(subset_query, fields=['title'])\n",
    "\n",
    "    # Using enumerate to show progress\n",
    "    for i, item in enumerate(search_results):\n",
    "        if 'title' in item:\n",
    "            full_title = item['title']\n",
    "\n",
    "            # This logic splits the title at the colon before the date\n",
    "            # e.g., \"Kentucky gazette (Lexington, Ky. : 1789): 1799-01-02\"\n",
    "            # becomes \"Kentucky gazette (Lexington, Ky. : 1789)\"\n",
    "            parts = full_title.split(':')\n",
    "            if len(parts) > 1:\n",
    "                # Join all parts except the last one (the date)\n",
    "                newspaper_name = \":\".join(parts[:-1]).strip()\n",
    "            else:\n",
    "                # Use the full title if no colon is found\n",
    "                newspaper_name = full_title.strip()\n",
    "\n",
    "            # Add the cleaned newspaper name to the set\n",
    "            unique_newspaper_titles.add(newspaper_name)\n",
    "\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"Processed {i+1} of {search_results.num_found} items...\")\n",
    "\n",
    "    print(f\"Search complete. Processed {i+1} total items.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during search: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 5: Save and Count Unique Titles\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 5: Save and Count Unique Titles ---\")\n",
    "\n",
    "try:\n",
    "    # Save the list of unique titles to the file\n",
    "    print(f\"Saving unique titles list to {unique_titles_list}...\")\n",
    "    with open(unique_titles_list, 'w', encoding='utf-8') as f:\n",
    "        # Sort the set for a clean, alphabetical list\n",
    "        for title in sorted(unique_newspaper_titles):\n",
    "            f.write(f\"{title}\\n\")\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "    print(\"\\n--- FINAL COUNT ---\")\n",
    "    # Print the count by getting the length of the set\n",
    "    print(f\"Total unique newspaper titles found: {len(unique_newspaper_titles)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "print(\"\\n--- All tasks finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uezc58PEONtf",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Step 1: Mount Google Drive ---\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive mounted successfully.\n",
      "\n",
      "--- Running Step 2: Downloading English Dictionary ---\n",
      "This is required for the script to 'guess' corrections.\n",
      "English dictionary loaded.\n",
      "\n",
      "--- Running Step 3: Configuring Paths & Counter ---\n",
      "Reading from: /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset\n",
      "Writing clean files to: /content/drive/MyDrive/KDN_Kernel_Downloads_Cleaned\n",
      "Writing correction map to: /content/drive/MyDrive/KDN_Archive_Downloads/auto_correction_map.txt\n",
      "\n",
      "--- Running Phase 1: Counting All Words ---\n",
      "Starting scan of /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset to find errors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...scanned 500 files...\n",
      "  ...scanned 1000 files...\n",
      "  ...scanned 1500 files...\n",
      "  ...scanned 2000 files...\n",
      "Scan complete. Found 2163205 unique words in 2474 files.\n",
      "Word count finished in 57.02 seconds.\n",
      "\n",
      "--- Running Phase 2: Automatically Building Correction Map ---\n",
      "Auto-generated 2493 correction rules.\n",
      "Saved correction map to /content/drive/MyDrive/KDN_Archive_Downloads/auto_correction_map.txt\n",
      "\n",
      "--- Running Phase 3: Cleaning All Files ---\n",
      "  ...cleaned 500 files...\n",
      "  ...cleaned 1000 files...\n",
      "  ...cleaned 1500 files...\n",
      "  ...cleaned 2000 files...\n",
      "Cleaning complete in 3979.44 seconds.\n",
      "Successfully cleaned and wrote: 2474 files.\n",
      "Skipped (already clean): 0 files.\n",
      "\n",
      "--- All tasks finished ---\n",
      "Your clean data is ready in: /content/drive/MyDrive/KDN_Kernel_Downloads_Cleaned\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# This script is designed to be run in a single Google Colab cell.\n",
    "# It AUTOMATES the entire cleaning process. It will:\n",
    "# 1. Scan all your GARBLED files to find common words.\n",
    "# 2. Automatically build a correction dictionary (e.g., \"fubfcriber\" -> \"subscriber\").\n",
    "# 3. Use that new dictionary to clean all your files and save them\n",
    "#    to a new \"Cleaned\" directory.\n",
    "#\n",
    "# This script REPLACES both kdn_word_counter.py and kdn_cleaner.py.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from google.colab import drive\n",
    "from collections import Counter\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 1: Mount your Google Drive\n",
    "# ------------------------------------------------------------------\n",
    "print(\"--- Running Step 1: Mount Google Drive ---\")\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting drive: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 2: Install and Download NLTK Dictionary\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 2: Downloading English Dictionary ---\")\n",
    "print(\"This is required for the script to 'guess' corrections.\")\n",
    "try:\n",
    "    nltk.download('words')\n",
    "    english_words = set(words.words())\n",
    "    print(\"English dictionary loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK dictionary: {e}\")\n",
    "    # We can't continue without the dictionary\n",
    "    raise e\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 3: Define Directories and Word Counter\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 3: Configuring Paths & Counter ---\")\n",
    "\n",
    "# This is the directory with your ORGANIZED but GARBLED files\n",
    "SOURCE_DIRECTORY = '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset'\n",
    "# This is the new directory where the CLEANED files will be saved\n",
    "CLEANED_DIRECTORY = '/content/drive/MyDrive/KDN_Kernel_Downloads_Cleaned'\n",
    "# This is where the auto-generated correction list will be saved\n",
    "CORRECTION_MAP_FILE = '/content/drive/MyDrive/KDN_Archive_Downloads/auto_correction_map.txt'\n",
    "\n",
    "print(f\"Reading from: {SOURCE_DIRECTORY}\")\n",
    "print(f\"Writing clean files to: {CLEANED_DIRECTORY}\")\n",
    "print(f\"Writing correction map to: {CORRECTION_MAP_FILE}\")\n",
    "os.makedirs(CLEANED_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# A regex to find words: sequences of alphabetic characters\n",
    "word_regex = re.compile(r'\\b[a-z]+\\b')\n",
    "\n",
    "def count_words(source_dir):\n",
    "    \"\"\"\n",
    "    Walks all subdirectories, reads all .txt files, counts all words.\n",
    "    \"\"\"\n",
    "    word_counter = Counter()\n",
    "    file_count = 0\n",
    "    print(f\"Starting scan of {source_dir} to find errors...\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        text_content = f.read()\n",
    "\n",
    "                    words = word_regex.findall(text_content.lower())\n",
    "                    word_counter.update(words)\n",
    "                    file_count += 1\n",
    "\n",
    "                    if file_count % 500 == 0:\n",
    "                        print(f\"  ...scanned {file_count} files...\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  ERROR: Could not process file {filename}. {e}\")\n",
    "\n",
    "    print(f\"Scan complete. Found {len(word_counter)} unique words in {file_count} files.\")\n",
    "    return word_counter\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 4: Phase 1 - Run Word Count\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Phase 1: Counting All Words ---\")\n",
    "start_time = time.time()\n",
    "word_counts = count_words(SOURCE_DIRECTORY)\n",
    "print(f\"Word count finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 5: Phase 2 - Auto-Build Correction Map\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Phase 2: Automatically Building Correction Map ---\")\n",
    "# This is the map we will build\n",
    "OCR_ERROR_MAP = {}\n",
    "# This is the minimum number of times a word must appear to be checked\n",
    "FREQUENCY_THRESHOLD = 5\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    # Only check words that appear often enough\n",
    "    if count < FREQUENCY_THRESHOLD:\n",
    "        continue\n",
    "\n",
    "    # Check for the \"long s\" (f) error\n",
    "    if 'f' in word and word not in english_words:\n",
    "        # Guess the correction by replacing 'f' with 's'\n",
    "        guessed_word = word.replace('f', 's')\n",
    "\n",
    "        # Check if our guess is a real English word\n",
    "        if guessed_word in english_words:\n",
    "            # We found a match! Add it to our map.\n",
    "            # We use regex to be case-insensitive later\n",
    "            regex_pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "            OCR_ERROR_MAP[regex_pattern] = guessed_word\n",
    "\n",
    "print(f\"Auto-generated {len(OCR_ERROR_MAP)} correction rules.\")\n",
    "\n",
    "# Save the map to a file so you can inspect it\n",
    "try:\n",
    "    with open(CORRECTION_MAP_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Auto-Generated OCR Correction Map\\n\")\n",
    "        f.write(\"----------------------------------\\n\")\n",
    "        for key, value in OCR_ERROR_MAP.items():\n",
    "            f.write(f\"'{key}'  ->  '{value}'\\n\")\n",
    "    print(f\"Saved correction map to {CORRECTION_MAP_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save correction map. {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 6: Phase 3 - Clean and Write Files\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Phase 3: Cleaning All Files ---\")\n",
    "start_time = time.time()\n",
    "cleaned_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "# Pre-compile the regex rules for speed\n",
    "# We create one giant regex that finds any of the error words\n",
    "# This is MUCH faster than looping through the dictionary for every file\n",
    "if OCR_ERROR_MAP:\n",
    "    regex_pattern = re.compile(\n",
    "        \"|\".join(OCR_ERROR_MAP.keys()),\n",
    "        re.IGNORECASE # Make it case-insensitive\n",
    "    )\n",
    "\n",
    "    # This function will be called by re.sub for every match\n",
    "    def find_replacement(match):\n",
    "        # Look up the lowercase version of the found word in our map\n",
    "        key = r'\\b' + re.escape(match.group(0).lower()) + r'\\b'\n",
    "        replacement = OCR_ERROR_MAP.get(key)\n",
    "\n",
    "        # This handles capitalization (e.g., \"Congrefs\" -> \"Congress\")\n",
    "        if replacement:\n",
    "            if match.group(0).isupper():\n",
    "                return replacement.upper()\n",
    "            elif match.group(0)[0].isupper():\n",
    "                return replacement.capitalize()\n",
    "            else:\n",
    "                return replacement\n",
    "        return match.group(0) # Should not happen, but a good fallback\n",
    "else:\n",
    "    print(\"No correction rules were generated. Files will be copied as-is.\")\n",
    "    regex_pattern = None\n",
    "\n",
    "# Now, walk the SOURCE directory and apply the cleaning\n",
    "for dirpath, dirnames, filenames in os.walk(SOURCE_DIRECTORY):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            source_file_path = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Create the matching directory structure in the CLEANED folder\n",
    "            relative_path = os.path.relpath(dirpath, SOURCE_DIRECTORY)\n",
    "            target_dir = os.path.join(CLEANED_DIRECTORY, relative_path)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            target_file_path = os.path.join(target_dir, filename)\n",
    "\n",
    "            # --- Resumability: Skip files we've already cleaned ---\n",
    "            if os.path.exists(target_file_path):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Read the garbled text\n",
    "                with open(source_file_path, 'r', encoding='utf-8', errors='ignore') as f_in:\n",
    "                    garbled_text = f_in.read()\n",
    "\n",
    "                # Clean the text\n",
    "                if regex_pattern:\n",
    "                    cleaned_text = regex_pattern.sub(find_replacement, garbled_text)\n",
    "                else:\n",
    "                    cleaned_text = garbled_text # No rules, just copy\n",
    "\n",
    "                # Write the new clean text\n",
    "                with open(target_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                    f_out.write(cleaned_text)\n",
    "\n",
    "                cleaned_count += 1\n",
    "\n",
    "                if cleaned_count % 500 == 0:\n",
    "                    print(f\"  ...cleaned {cleaned_count} files...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not clean file {filename}. {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Cleaning complete in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Successfully cleaned and wrote: {cleaned_count} files.\")\n",
    "print(f\"Skipped (already clean): {skipped_count} files.\")\n",
    "\n",
    "print(\"\\n--- All tasks finished ---\")\n",
    "print(f\"Your clean data is ready in: {CLEANED_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "gVzoMMpi1Bqi",
    "language": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "# This script is designed to be run in a single Google Colab cell.\n",
    "# It will download a subset of the 'kentuckynewspapers' collection\n",
    "# for items related to \"Lexington\".\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 1: Mount your Google Drive\n",
    "# ------------------------------------------------------------------\n",
    "print(\"--- Running Step 1: Mount Google Drive ---\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting drive: {e}\")\n",
    "    print(\"Please ensure you are running this in Google Colab.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 2: Install 'internetarchive' (if needed)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 2: Check/Install 'internetarchive' library ---\")\n",
    "try:\n",
    "    import internetarchive\n",
    "    print(\"internetarchive library is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"internetarchive library not found. Installing...\")\n",
    "    # The '!' character runs this as a shell command in Colab\n",
    "    !pip install internetarchive\n",
    "    print(\"Installation complete.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 3: Set up your download directory\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 3: Set up Download Directory ---\")\n",
    "import os\n",
    "\n",
    "# We will create a new, specific folder for this subset\n",
    "output_directory = '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset'\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "print(f\"Files will be saved to: {output_directory}\")\n",
    "\n",
    "# This is the correct file format for \"DjVuTXT\"\n",
    "glob_pattern = \"*_djvu.txt\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cell 4: Download the \"Lexington\" Subset\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Running Step 4: Download Lexington Subset ---\")\n",
    "\n",
    "print(\"Starting download for a subset: 'Lexington'...\")\n",
    "\n",
    "# This query searches for items in the collection that have\n",
    "# \"Lexington\" in their title or in the 'place_of_publication' field.\n",
    "# Using single quotes for the shell command is safer.\n",
    "subset_query = 'collection:kentuckynewspapers AND (place_of_publication:\"Lexington\" OR title:\"Lexington\")'\n",
    "\n",
    "\n",
    "# Run the download command for the subset\n",
    "# This command first searches for all items matching the query,\n",
    "# generates a list of them (--itemlist), and then pipes (|)\n",
    "# that list to 'ia download' which reads the list from stdin (--itemlist=-).\n",
    "print(f\"Starting search... this may take a moment. Will download files matching {glob_pattern}\")\n",
    "\n",
    "# Note: The 'ia' tool is resumable. If this is interrupted,\n",
    "# you can just run it again to continue.\n",
    "!ia search '{subset_query}' --itemlist | ia download --itemlist=- --destdir=\"{output_directory}\" --glob=\"{glob_pattern}\"\n",
    "\n",
    "print(\"\\n--- Lexington Subset Download Complete ---\")\n",
    "print(f\"Check the folder '{output_directory}' in your Google Drive.\")\n",
    "print(\"\\n--- All tasks finished ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFgcxY562l-w",
    "outputId": "1c869c03-13f9-4d11-9860-f6f88a92171e",
    "cellView": "form",
    "collapsed": true,
    "language": "python"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Running Step 1: Mount Google Drive ---\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive mounted successfully.\n",
      "\n",
      "--- Running Step 2: Check/Install 'internetarchive' library ---\n",
      "internetarchive library is already installed.\n",
      "\n",
      "--- Running Step 3: Set up Download Directory ---\n",
      "Files will be saved to: /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset\n",
      "\n",
      "--- Running Step 4: Download Lexington Subset ---\n",
      "Starting download for a subset: 'Lexington'...\n",
      "Starting search... this may take a moment. Will download files matching *_djvu.txt\n",
      "kd9x639z9k7x (1/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/kd9x639z9k7x/kd9x639z9k7x_djvu.txt, file already exists based on length and date.\n",
      "xt700000095d (2/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt700000095d/xt700000095d_djvu.txt, file already exists based on length and date.\n",
      "xt7000000960 (3/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt7000000960/xt7000000960_djvu.txt, file already exists based on length and date.\n",
      "xt700000097k (4/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt700000097k/xt700000097k_djvu.txt, file already exists based on length and date.\n",
      "xt700000099r (5/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt700000099r/xt700000099r_djvu.txt, file already exists based on length and date.\n",
      "xt7000000b1m (6/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt7000000b1m/xt7000000b1m_djvu.txt, file already exists based on length and date.\n",
      "xt7000000p9t (7/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt7000000p9t/xt7000000p9t_djvu.txt, file already exists based on length and date.\n",
      "xt7000000q03 (8/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt7000000q03/xt7000000q03_djvu.txt, file already exists based on length and date.\n",
      "xt7000000q28 (9/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt7000000q28/xt7000000q28_djvu.txt, file already exists based on length and date.\n",
      "xt7000002c3c (10/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt7000002c3c/xt7000002c3c_djvu.txt, file already exists based on length and date.\n",
      "xt702v2c8j1c (11/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt702v2c8j1c/xt702v2c8j1c_djvu.txt, file already exists based on length and date.\n",
      "xt702v2c8j3j (12/2478):\n",
      " skipping /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt702v2c8j3j/xt702v2c8j3j_djvu.txt, file already exists based on length and date.\n",
      "xt702v2c8j44 (13/2478):\n",
      "^C\n",
      "\n",
      "--- Lexington Subset Download Complete ---\n",
      "Check the folder '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset' in your Google Drive.\n",
      "\n",
      "--- All tasks finished ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6a73cb6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762562894374,
     "user_tz": 300,
     "elapsed": 23,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     }
    },
    "outputId": "1811eb1d-d69d-4616-df81-c2e003a2d62d",
    "language": "python"
   },
   "source": [
    "import os\n",
    "\n",
    "# The output_directory is already defined in the previous cell.\n",
    "# For clarity and to make this cell runnable independently if needed,\n",
    "# I will redefine it here, assuming the previous cell has been run.\n",
    "output_directory = '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset'\n",
    "\n",
    "# Get all entries in the directory\n",
    "all_entries = os.listdir(output_directory)\n",
    "\n",
    "# Filter for files ending with '.txt'\n",
    "downloaded_files = [entry for entry in all_entries if entry.endswith('.txt')]\n",
    "\n",
    "# Print the list of downloaded text files\n",
    "print(\"Downloaded text files:\")\n",
    "for file in downloaded_files:\n",
    "    print(file)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloaded text files:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "733a62d4",
    "language": "markdown"
   },
   "source": [
    "## Get the full item list\n",
    "\n",
    "### Subtask:\n",
    "Obtain the complete list of items that should be downloaded based on the search query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86002c4e",
    "language": "markdown"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the search query and use the `ia search` command to get the list of item identifiers into a Python variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "24e97fb4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762562910086,
     "user_tz": 300,
     "elapsed": 1414,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     }
    },
    "outputId": "69092c84-b937-4e25-89a6-0e39eedaab2b",
    "language": "python"
   },
   "source": [
    "import subprocess\n",
    "\n",
    "subset_query = 'collection:kentuckynewspapers AND (place_of_publication:\"Lexington\" OR title:\"Lexington\")'\n",
    "\n",
    "# Use subprocess to run the ia search command and capture its output\n",
    "try:\n",
    "    # The output is likely large, so using capture_output=True and text=True is suitable.\n",
    "    # Setting a timeout to prevent hanging indefinitely.\n",
    "    result = subprocess.run(\n",
    "        ['ia', 'search', subset_query, '--itemlist'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True, # Raise CalledProcessError for bad exit codes\n",
    "        timeout=600 # 10 minutes timeout\n",
    "    )\n",
    "    item_list_output = result.stdout\n",
    "    print(\"Successfully obtained item list from ia search.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ia' command not found. Ensure internetarchive is installed and in your PATH.\")\n",
    "    item_list_output = \"\" # Set to empty to avoid errors in the next step\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error executing ia search: {e}\")\n",
    "    print(f\"Stderr: {e.stderr}\")\n",
    "    item_list_output = \"\" # Set to empty to avoid errors in the next step\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"Error: ia search command timed out.\")\n",
    "    item_list_output = \"\" # Set to empty to avoid errors in the next step\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    item_list_output = \"\" # Set to empty to avoid errors in the next step\n",
    "\n",
    "# Split the captured output into a list of item identifiers\n",
    "# Strip any leading/trailing whitespace from each item\n",
    "item_identifiers = [item.strip() for item in item_list_output.splitlines() if item.strip()]\n",
    "\n",
    "print(f\"Found {len(item_identifiers)} items matching the query.\")\n",
    "# Display the first few identifiers as a check\n",
    "print(\"First 10 item identifiers:\")\n",
    "display(item_identifiers[:10])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully obtained item list from ia search.\n",
      "Found 2478 items matching the query.\n",
      "First 10 item identifiers:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['kd9x639z9k7x',\n",
       " 'xt700000095d',\n",
       " 'xt7000000960',\n",
       " 'xt700000097k',\n",
       " 'xt700000099r',\n",
       " 'xt7000000b1m',\n",
       " 'xt7000000p9t',\n",
       " 'xt7000000q03',\n",
       " 'xt7000000q28',\n",
       " 'xt7000002c3c']"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eb43398",
    "language": "markdown"
   },
   "source": [
    "## Identify remaining items\n",
    "\n",
    "### Subtask:\n",
    "Compare the list of downloaded files with the full item list to determine which items still need to be downloaded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a259931f",
    "language": "markdown"
   },
   "source": [
    "**Reasoning**:\n",
    "Implement steps 1-8 of the instructions to compare the full item list with the downloaded files and identify the remaining items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "ed36fc31",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762562925231,
     "user_tz": 300,
     "elapsed": 22,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     }
    },
    "outputId": "3a966a59-8afb-4362-e724-50fe52b569e3",
    "language": "python"
   },
   "source": [
    "# 1. Create an empty list called downloaded_item_ids\n",
    "downloaded_item_ids = []\n",
    "\n",
    "# 2. Iterate through the downloaded_files list\n",
    "# 3. For each file name, extract the item ID and append to downloaded_item_ids\n",
    "for file_name in downloaded_files:\n",
    "    # Split the filename at the first underscore\n",
    "    item_id = file_name.split('_', 1)[0]\n",
    "    downloaded_item_ids.append(item_id)\n",
    "\n",
    "# 4. Create a set of downloaded_item_ids for efficient lookup\n",
    "downloaded_item_ids_set = set(downloaded_item_ids)\n",
    "\n",
    "# 5. Create a list called remaining_item_ids\n",
    "remaining_item_ids = []\n",
    "\n",
    "# 6. Iterate through the item_identifiers list\n",
    "# 7. For each item ID in item_identifiers, check if it is present in the set\n",
    "# 8. If the item ID is NOT in the set, append it to the remaining_item_ids list\n",
    "for item_id in item_identifiers:\n",
    "    if item_id not in downloaded_item_ids_set:\n",
    "        remaining_item_ids.append(item_id)\n",
    "\n",
    "# 9. Print the number of remaining items and the first few items\n",
    "print(f\"Number of remaining items to download: {len(remaining_item_ids)}\")\n",
    "print(\"First 10 remaining item identifiers:\")\n",
    "display(remaining_item_ids[:10])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of remaining items to download: 2478\n",
      "First 10 remaining item identifiers:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['kd9x639z9k7x',\n",
       " 'xt700000095d',\n",
       " 'xt7000000960',\n",
       " 'xt700000097k',\n",
       " 'xt700000099r',\n",
       " 'xt7000000b1m',\n",
       " 'xt7000000p9t',\n",
       " 'xt7000000q03',\n",
       " 'xt7000000q28',\n",
       " 'xt7000002c3c']"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "595cab17",
    "language": "markdown"
   },
   "source": [
    "## Resume download\n",
    "\n",
    "### Subtask:\n",
    "Use the identified list of remaining items to resume the download using `ia download`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0a12da6",
    "language": "markdown"
   },
   "source": [
    "**Reasoning**:\n",
    "Execute the `ia download` command using `subprocess.run` to download the remaining items, incorporating necessary arguments for destination, glob pattern, retries, and error handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "ef6c1003",
    "executionInfo": {
     "status": "error",
     "timestamp": 1762570366969,
     "user_tz": 300,
     "elapsed": 8561,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     }
    },
    "outputId": "9aa3bbad-2d96-490b-f397-675b606f8ae6",
    "language": "python"
   },
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import requests # Import the requests library for making API calls\n",
    "\n",
    "# Define the output directory and glob pattern as used previously\n",
    "output_directory = '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset'\n",
    "glob_pattern = \"*_djvu.txt\"\n",
    "\n",
    "# Get a list of downloaded files to identify the last one\n",
    "try:\n",
    "    all_entries = os.listdir(output_directory)\n",
    "    downloaded_files = sorted([os.path.join(output_directory, entry) for entry in all_entries if entry.endswith('.txt')], key=os.path.getmtime)\n",
    "\n",
    "    last_downloaded_file = downloaded_files[-1] if downloaded_files else None\n",
    "\n",
    "    if last_downloaded_file:\n",
    "        last_downloaded_item_id = os.path.basename(last_downloaded_file).split('_', 1)[0]\n",
    "        print(f\"Last downloaded file identified: {last_downloaded_file}\")\n",
    "        print(f\"Last downloaded item ID: {last_downloaded_item_id}\")\n",
    "    else:\n",
    "        last_downloaded_item_id = None\n",
    "        print(\"No previous downloads found in the directory.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error identifying last downloaded file: {e}\")\n",
    "    last_downloaded_item_id = None\n",
    "\n",
    "# Find the index of the last downloaded item in the full item list\n",
    "start_index = 0\n",
    "if last_downloaded_item_id and item_identifiers:\n",
    "    try:\n",
    "        start_index = item_identifiers.index(last_downloaded_item_id) + 1\n",
    "        print(f\"Resuming download from item index: {start_index} (after {last_downloaded_item_id})\")\n",
    "    except ValueError:\n",
    "        print(f\"Last downloaded item ID '{last_downloaded_item_id}' not found in the full item list. Starting from the beginning.\")\n",
    "        start_index = 0\n",
    "\n",
    "remaining_item_ids_for_download = item_identifiers[start_index:]\n",
    "total_items_to_download_this_session = len(remaining_item_ids_for_download)\n",
    "print(f\"Starting download of {total_items_to_download_this_session} remaining items...\")\n",
    "\n",
    "batch_size = 10\n",
    "download_counter = start_index # Initialize counter based on where we are resuming\n",
    "\n",
    "for i in range(0, total_items_to_download_this_session, batch_size):\n",
    "    batch_item_ids = remaining_item_ids_for_download[i:i + batch_size]\n",
    "    current_batch_num = (i // batch_size) + 1\n",
    "    total_batches = (total_items_to_download_this_session + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"\\n--- Processing Batch {current_batch_num}/{total_batches} ({len(batch_item_ids)} items) ---\")\n",
    "\n",
    "    # Check if we should get newspaper/issue info for this batch\n",
    "    if (download_counter + len(batch_item_ids)) // 50 > download_counter // 50:\n",
    "        print(\"Fetching newspaper and issue information...\")\n",
    "        for item_id in batch_item_ids:\n",
    "            # Construct the API URL for metadata\n",
    "            metadata_url = f\"https://archive.org/metadata/{item_id}\"\n",
    "            try:\n",
    "                response = requests.get(metadata_url)\n",
    "                response.raise_for_status() # Raise an exception for bad status codes\n",
    "                metadata = response.json()\n",
    "\n",
    "                title = metadata.get('metadata', {}).get('title', 'N/A')\n",
    "                date = metadata.get('metadata', {}).get('date', 'N/A')\n",
    "\n",
    "                print(f\"  - Item ID: {item_id}, Title: {title}, Date: {date}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  - Error fetching metadata for {item_id}: {e}\")\n",
    "\n",
    "    # Prepare the batch item IDs for piping to standard input\n",
    "    batch_items_input = \"\\n\".join(batch_item_ids)\n",
    "\n",
    "    # Construct the ia download command for the current batch\n",
    "    download_command = [\n",
    "        'ia',\n",
    "        'download',\n",
    "        '--itemlist=-',\n",
    "        f'--destdir={output_directory}',\n",
    "        f'--glob={glob_pattern}',\n",
    "        '--retries', '5'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(\n",
    "            download_command,\n",
    "            input=batch_items_input,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            check=True,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        batch_duration = end_time - start_time\n",
    "\n",
    "        print(f\"Batch {current_batch_num} completed in {batch_duration:.2f} seconds.\")\n",
    "        # Optional: Print stdout/stderr if needed for debugging\n",
    "        # if result.stdout:\n",
    "        #     print(\"Batch Stdout:\")\n",
    "        #     print(result.stdout)\n",
    "        # if result.stderr:\n",
    "        #     print(\"Batch Stderr:\")\n",
    "        #     print(result.stderr)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 'ia' command not found during batch {current_batch_num}. Ensure internetarchive is installed and in your PATH.\")\n",
    "        break\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error executing ia download for batch {current_batch_num}: {e}\")\n",
    "        print(f\"Stdout: {e.stdout}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Error: ia download command timed out for batch {current_batch_num}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during batch {current_batch_num}: {e}\")\n",
    "\n",
    "    download_counter += len(batch_item_ids) # Increment counter after processing the batch\n",
    "\n",
    "print(\"\\n--- All Batches Processed ---\")\n",
    "print(f\"Check the folder '{output_directory}' in your Google Drive.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No previous downloads found in the directory.\n",
      "Starting download of 2478 remaining items...\n",
      "\n",
      "--- Processing Batch 1/248 (10 items) ---\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-274423644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         result = subprocess.run(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mdownload_command\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_items_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e909be74-2311-4f9d-a991-5944c3c01a6a",
    "id": "BXv6shT6YYdC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762570988615,
     "user_tz": 300,
     "elapsed": 363895,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     }
    },
    "language": "python"
   },
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import requests # Import the requests library for making API calls\n",
    "\n",
    "# Define the output directory and glob pattern as used previously\n",
    "output_directory = '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset'\n",
    "glob_pattern = \"*_djvu.txt\"\n",
    "\n",
    "# Add explicit checks and print statements for the output directory\n",
    "print(f\"Checking directory: {output_directory}\")\n",
    "if os.path.exists(output_directory):\n",
    "    print(\"Output directory exists.\")\n",
    "    # Optional: Add a small delay to allow file system to sync\n",
    "    # time.sleep(2)\n",
    "\n",
    "    # Get a list of downloaded files to identify the last one\n",
    "    try:\n",
    "        downloaded_files = []\n",
    "        # List all entries in the main download directory (expected to be item ID directories)\n",
    "        all_entries = os.listdir(output_directory)\n",
    "        print(f\"Entries found in main directory (expected item IDs): {len(all_entries)}\")\n",
    "\n",
    "        for entry in all_entries:\n",
    "            item_dir = os.path.join(output_directory, entry)\n",
    "            # Check if the entry is a directory\n",
    "            if os.path.isdir(item_dir):\n",
    "                # List files within the item ID directory\n",
    "                files_in_item_dir = os.listdir(item_dir)\n",
    "                # Filter for the expected glob pattern\n",
    "                for file_name in files_in_item_dir:\n",
    "                    if file_name.endswith('_djvu.txt'): # More specific check than just .txt\n",
    "                        downloaded_files.append(os.path.join(item_dir, file_name))\n",
    "\n",
    "        # Sort the found files by modification time to find the last downloaded\n",
    "        downloaded_files.sort(key=os.path.getmtime)\n",
    "\n",
    "        print(f\"Filtered downloaded files (.txt): {len(downloaded_files)}\")\n",
    "        if downloaded_files:\n",
    "            last_downloaded_file = downloaded_files[-1]\n",
    "            # Extract the item ID from the last downloaded file name\n",
    "            last_downloaded_item_id = os.path.basename(last_downloaded_file).split('_', 1)[0]\n",
    "            print(f\"Last downloaded file identified: {last_downloaded_file}\")\n",
    "            print(f\"Last downloaded item ID: {last_downloaded_item_id}\")\n",
    "        else:\n",
    "            last_downloaded_item_id = None\n",
    "            print(\"No previous .txt downloads found in the directory or its subdirectories.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying last downloaded file: {e}\")\n",
    "        last_downloaded_item_id = None\n",
    "\n",
    "\n",
    "# Find the index of the last downloaded item in the full item list\n",
    "start_index = 0\n",
    "if last_downloaded_item_id and item_identifiers:\n",
    "    try:\n",
    "        start_index = item_identifiers.index(last_downloaded_item_id) + 1\n",
    "        print(f\"Resuming download from item index: {start_index} (after {last_downloaded_item_id})\")\n",
    "    except ValueError:\n",
    "        print(f\"Last downloaded item ID '{last_downloaded_item_id}' not found in the full item list. Starting from the beginning.\")\n",
    "        start_index = 0\n",
    "\n",
    "# Get the list of items remaining to download starting from the identified index\n",
    "remaining_item_ids_for_download = item_identifiers[start_index:]\n",
    "\n",
    "# Determine the total number of items to download in this session\n",
    "total_items_to_download_this_session = len(remaining_item_ids_for_download)\n",
    "print(f\"Starting download of {total_items_to_download_this_session} remaining items...\")\n",
    "\n",
    "# Define the batch size for processing\n",
    "batch_size = 10 # You can adjust this number\n",
    "\n",
    "# Initialize download counter to keep track of overall progress\n",
    "download_counter = start_index\n",
    "\n",
    "# Iterate through the remaining item IDs in batches\n",
    "for i in range(0, total_items_to_download_this_session, batch_size):\n",
    "    batch_item_ids = remaining_item_ids_for_download[i:i + batch_size]\n",
    "    current_batch_num = (i // batch_size) + 1\n",
    "    total_batches = (total_items_to_download_this_session + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"\\n--- Processing Batch {current_batch_num}/{total_batches} ({len(batch_item_ids)} items) ---\")\n",
    "\n",
    "    # Get and display newspaper/issue info for the first item in the batch\n",
    "    if batch_item_ids: # Ensure batch is not empty\n",
    "        first_item_id_in_batch = batch_item_ids[0]\n",
    "        metadata_url = f\"https://archive.org/metadata/{first_item_id_in_batch}\"\n",
    "        try:\n",
    "            response = requests.get(metadata_url)\n",
    "            response.raise_for_status()\n",
    "            metadata = response.json()\n",
    "\n",
    "            title = metadata.get('metadata', {}).get('title', 'N/A')\n",
    "            date = metadata.get('metadata', {}).get('date', 'N/A')\n",
    "\n",
    "            print(f\"  - Currently processing: Item ID: {first_item_id_in_batch}, Title: {title}, Date: {date}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  - Error fetching metadata for {first_item_id_in_batch}: {e}\")\n",
    "\n",
    "\n",
    "    # Prepare the batch item IDs for piping to standard input\n",
    "    batch_items_input = \"\\n\".join(batch_item_ids)\n",
    "\n",
    "    # Construct the ia download command for the current batch\n",
    "    download_command = [\n",
    "        'ia',\n",
    "        'download',\n",
    "        '--itemlist=-',\n",
    "        f'--destdir={output_directory}',\n",
    "        f'--glob={glob_pattern}',\n",
    "        '--retries', '5'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(\n",
    "            download_command,\n",
    "            input=batch_items_input,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            check=True,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        batch_duration = end_time - start_time\n",
    "\n",
    "        print(f\"Batch {current_batch_num} completed in {batch_duration:.2f} seconds.\")\n",
    "        # Optional: Print stdout/stderr if needed for debugging\n",
    "        # if result.stdout:\n",
    "        #     print(\"Batch Stdout:\")\n",
    "        #     print(result.stdout)\n",
    "        # if result.stderr:\n",
    "        #     print(\"Batch Stderr:\")\n",
    "        #     print(result.stderr)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 'ia' command not found during batch {current_batch_num}. Ensure internetarchive is installed and in your PATH.\")\n",
    "        break\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error executing ia download for batch {current_batch_num}: {e}\")\n",
    "        print(f\"Stdout: {e.stdout}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Error: ia download command timed out for batch {current_batch_num}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during batch {current_batch_num}: {e}\")\n",
    "\n",
    "    # Update download_counter based on the number of items processed in the batch\n",
    "    download_counter = start_index + i + len(batch_item_ids)\n",
    "\n",
    "\n",
    "print(\"\\n--- All Batches Processed ---\")\n",
    "print(f\"Check the folder '{output_directory}' in your Google Drive.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking directory: /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset\n",
      "Output directory exists.\n",
      "Entries found in main directory (expected item IDs): 2478\n",
      "Filtered downloaded files (.txt): 2474\n",
      "Last downloaded file identified: /content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset/xt7wdb7vnb2w/xt7wdb7vnb2w_djvu.txt\n",
      "Last downloaded item ID: xt7wdb7vnb2w\n",
      "Resuming download from item index: 2283 (after xt7wdb7vnb2w)\n",
      "Starting download of 195 remaining items...\n",
      "\n",
      "--- Processing Batch 1/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7wdb7vnb3g, Title: Kentucky gazette (Lexington, Ky. : 1789): 1799-01-02, Date: 1799-01-02\n",
      "Batch 1 completed in 18.15 seconds.\n",
      "\n",
      "--- Processing Batch 2/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7wm32n6r3m, Title: Kentucky gazette (Lexington, Ky. : 1809): 1836-12-29, Date: 1836-12-29\n",
      "Batch 2 completed in 21.22 seconds.\n",
      "\n",
      "--- Processing Batch 3/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7wpz51gz6k, Title: Kentucky gazette (Lexington, Ky. : 1789): 1801-06-08, Date: 1801-06-08\n",
      "Batch 3 completed in 16.11 seconds.\n",
      "\n",
      "--- Processing Batch 4/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7wst7ds64m, Title: Kentucky gazette (Lexington, Ky. : 1809): 1839-04-11, Date: 1839-04-11\n",
      "Batch 4 completed in 14.42 seconds.\n",
      "\n",
      "--- Processing Batch 5/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7wwp9t2s97, Title: Blue-grass blade (Lexington, Ky.): 1907-03-10, Date: 1907-03-10\n",
      "Batch 5 completed in 16.96 seconds.\n",
      "\n",
      "--- Processing Batch 6/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7x3f4kmt95, Title: Kentucky gazette (Lexington, Ky. : 1809): 1836-08-08, Date: 1836-08-08\n",
      "Batch 6 completed in 22.48 seconds.\n",
      "\n",
      "--- Processing Batch 7/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7x696zx26m, Title: Kentucky gazette (Lexington, Ky. : 1789): 1792-04-28, Date: 1792-04-28\n",
      "Batch 7 completed in 18.88 seconds.\n",
      "\n",
      "--- Processing Batch 8/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7x959c6878, Title: Kentucky gazette (Lexington, Ky. : 1789): 1794-07-12, Date: 1794-07-12\n",
      "Batch 8 completed in 17.63 seconds.\n",
      "\n",
      "--- Processing Batch 9/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7xd21rgh1c, Title: Kentucky gazette (Lexington, Ky. : 1789): 1802-10-05, Date: 1802-10-05\n",
      "Batch 9 completed in 16.25 seconds.\n",
      "\n",
      "--- Processing Batch 10/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7xks6j1x4z, Title: Kentucky gazette (Lexington, Ky. : 1809): 1826-03-03, Date: 1826-03-03\n",
      "Batch 10 completed in 22.40 seconds.\n",
      "\n",
      "--- Processing Batch 11/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7xpn8xb456, Title: Kentucky gazette (Lexington, Ky. : 1789): 1790-12-18, Date: 1790-12-18\n",
      "Batch 11 completed in 14.40 seconds.\n",
      "\n",
      "--- Processing Batch 12/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7xsj19mc58, Title: Kentucky gazette (Lexington, Ky. : 1809): 1839-11-14, Date: 1839-11-14\n",
      "Batch 12 completed in 18.18 seconds.\n",
      "\n",
      "--- Processing Batch 13/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7xwd3pwz87, Title: Blue-grass blade (Lexington, Ky.): 1903-11-29, Date: 1903-11-29\n",
      "Batch 13 completed in 16.14 seconds.\n",
      "\n",
      "--- Processing Batch 14/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7z348gg11p, Title: Kentucky gazette (Lexington, Ky. : 1809): 1815-02-27, Date: 1815-02-27\n",
      "Batch 14 completed in 20.17 seconds.\n",
      "\n",
      "--- Processing Batch 15/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7z610vr778, Title: Kentucky gazette (Lexington, Ky. : 1809): 1826-07-14, Date: 1826-07-14\n",
      "Batch 15 completed in 18.52 seconds.\n",
      "\n",
      "--- Processing Batch 16/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7z8w381t71, Title: Blue-grass blade (Lexington, Ky.): 1909-06-27, Date: 1909-06-27\n",
      "Batch 16 completed in 18.72 seconds.\n",
      "\n",
      "--- Processing Batch 17/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7zgm81kw1q, Title: Kentucky gazette (Lexington, Ky. : 1789): 1796-05-14, Date: 1796-05-14\n",
      "Batch 17 completed in 12.56 seconds.\n",
      "\n",
      "--- Processing Batch 18/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7zkh0dw367, Title: Kentucky gazette (Lexington, Ky. : 1789): 1789-10-31, Date: 1789-10-31\n",
      "Batch 18 completed in 20.67 seconds.\n",
      "\n",
      "--- Processing Batch 19/20 (10 items) ---\n",
      "  - Currently processing: Item ID: xt7zs756fj0g, Title: Kentucky gazette (Lexington, Ky. : 1809): 1815-06-12, Date: 1815-06-12\n",
      "Batch 19 completed in 18.10 seconds.\n",
      "\n",
      "--- Processing Batch 20/20 (5 items) ---\n",
      "  - Currently processing: Item ID: xt7zw37kqr3c, Title: Kentucky gazette (Lexington, Ky. : 1809): 1837-02-23, Date: 1837-02-23\n",
      "Batch 20 completed in 8.61 seconds.\n",
      "\n",
      "--- All Batches Processed ---\n",
      "Check the folder '/content/drive/MyDrive/KDN_Archive_Downloads/Lexington_Subset' in your Google Drive.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbHmnK3HAQhR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762580845866,
     "user_tz": 300,
     "elapsed": 17194,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     }
    },
    "outputId": "b17055b6-49b3-4fb8-a9b8-802af412752c",
    "language": "python"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48364330",
    "language": "markdown"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous `subprocess.run` command failed with a non-zero exit status (1), but the stderr was None, which is not helpful. To understand the cause of the error, I need to execute the command again, but this time capture the stderr output.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}