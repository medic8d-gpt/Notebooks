{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86vnBJPeXAhF"
   },
   "source": [
    "scans for dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xeckb84W-Ku"
   },
   "outputs": [],
   "source": [
    "# @title  Find and Move Duplicates --- IGNORE ---   and keeps the oldest version moves the rest to a 'dupeshit' folder\n",
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Calculates the SHA-256 hash of a file.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_and_move_duplicates():\n",
    "    \"\"\"\n",
    "    Finds duplicate files in the current directory and all subdirectories.\n",
    "    Keeps the file with the oldest modification time and moves the rest\n",
    "    to a 'dupeshit' folder.\n",
    "    \"\"\"\n",
    "\n",
    "    start_dir = os.getcwd()\n",
    "    dupes_dir = os.path.join(start_dir, \"dupeshit\")\n",
    "\n",
    "    # --- 1. Setup ---\n",
    "    try:\n",
    "        os.makedirs(dupes_dir, exist_ok=True)\n",
    "        print(f\"Duplicates will be moved to: {dupes_dir}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {dupes_dir}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Group files by size (fast pre-filter) ---\n",
    "    size_groups = defaultdict(list)\n",
    "    print(f\"Scanning files in {start_dir}...\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        # Don't scan the 'dupeshit' folder itself\n",
    "        if dirpath.startswith(dupes_dir):\n",
    "            continue\n",
    "\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                # Use os.path.realpath to follow symlinks\n",
    "                real_path = os.path.realpath(filepath)\n",
    "                if not os.path.isfile(real_path):\n",
    "                    continue\n",
    "\n",
    "                file_size = os.path.getsize(real_path)\n",
    "                if file_size > 0: # Ignore empty files\n",
    "                    size_groups[file_size].append(real_path)\n",
    "            except FileNotFoundError:\n",
    "                continue # Skip broken links\n",
    "\n",
    "    # --- 3. Group by content hash (slower, accurate check) ---\n",
    "    hash_groups = defaultdict(list)\n",
    "    total_files_checked = 0\n",
    "\n",
    "    print(\"Checking for duplicate content...\")\n",
    "    for size, files in size_groups.items():\n",
    "        if len(files) < 2:\n",
    "            total_files_checked += 1\n",
    "            continue # Not a duplicate if only one file of this size\n",
    "\n",
    "        for filepath in files:\n",
    "            file_hash = get_file_hash(filepath)\n",
    "            if file_hash:\n",
    "                hash_groups[file_hash].append(filepath)\n",
    "            total_files_checked += 1\n",
    "\n",
    "    # --- 4. Find oldest and move the rest ---\n",
    "    total_moved = 0\n",
    "    total_kept = 0\n",
    "\n",
    "    print(\"\\n--- Processing Duplicates ---\")\n",
    "\n",
    "    for file_hash, files in hash_groups.items():\n",
    "        if len(files) < 2:\n",
    "            total_kept += 1\n",
    "            continue # Unique file\n",
    "\n",
    "        # We have true duplicates. Now, find the oldest.\n",
    "        # Create a list of (modification_time, filepath) tuples\n",
    "        file_times = []\n",
    "        for filepath in files:\n",
    "            try:\n",
    "                mtime = os.path.getmtime(filepath)\n",
    "                file_times.append((mtime, filepath))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        # Sort by time (oldest first)\n",
    "        file_times.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Keep the first file (the oldest)\n",
    "        file_to_keep = file_times[0][1]\n",
    "        print(f\"\\nGroup (Hash: {file_hash[:10]}...):\")\n",
    "        print(f\"  Keeping (Oldest): {file_to_keep}\")\n",
    "        total_kept += 1\n",
    "\n",
    "        # Move all the others\n",
    "        dupes_to_move = file_times[1:]\n",
    "        for mtime, filepath in dupes_to_move:\n",
    "            try:\n",
    "                filename = os.path.basename(filepath)\n",
    "                dest_path = os.path.join(dupes_dir, filename)\n",
    "\n",
    "                # Handle name conflicts in the 'dupeshit' folder\n",
    "                counter = 1\n",
    "                while os.path.exists(dest_path):\n",
    "                    name, ext = os.path.splitext(filename)\n",
    "                    dest_path = os.path.join(dupes_dir, f\"{name}_{counter}{ext}\")\n",
    "                    counter += 1\n",
    "\n",
    "                shutil.move(filepath, dest_path)\n",
    "                print(f\"  Moved:            {filepath}\")\n",
    "                total_moved += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error moving {filepath}: {e}\")\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Total Files Scanned: {total_files_checked}\")\n",
    "    print(f\"Unique Files Kept:   {total_kept}\")\n",
    "    print(f\"Duplicates Moved:    {total_moved}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_move_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qT1Hrd8oTe-Y"
   },
   "outputs": [],
   "source": [
    "# @title then run this to see sizes of files and store in nnnn.txt\n",
    "!ls -ls > nnnn.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "sbFUnbHQDHxf"
   },
   "outputs": [],
   "source": [
    "# @title sort the list by the file count (the first item in our tuple\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "search_path = \"/content/drive/MyDrive/main_shit\"\n",
    "dir_file_counts = []\n",
    "\n",
    "# os.walk goes through every directory and subdirectory\n",
    "# For each directory, it gives us the (dirpath, list_of_subdirs, list_of_files)\n",
    "for dirpath, dirnames, filenames in os.walk(search_path, topdown=True):\n",
    "\n",
    "    # We just need the count of files in the current directory\n",
    "    file_count = len(filenames)\n",
    "    dir_file_counts.append((file_count, dirpath))\n",
    "\n",
    "# Sort the list by the file count (the first item in our tuple)\n",
    "dir_file_counts.sort(key=lambda item: item[0])\n",
    "\n",
    "# Print the sorted results\n",
    "print(\"--- File Counts per Directory ---\")\n",
    "for count, path in dir_file_counts:\n",
    "    print(f\"{count} {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB_JXjCIX5OZ"
   },
   "source": [
    "scan for (1) (2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "lIYdn1woOS1n"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_duplicates_by_filename(file_list_path, dest_folder=\"mddupes\"):\n",
    "    \"\"\"\n",
    "    Reads a text file containing a list of filenames and moves any\n",
    "    file with a ' (n)' suffix to the destination folder.\n",
    "\n",
    "    This script *only* looks at the filename pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Setup ---\n",
    "    try:\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        print(f\"Using destination folder: '{dest_folder}/'\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {dest_folder}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Regex to find suffixes like ' (1)', ' (2)', etc.\n",
    "    # This will match the file to be moved.\n",
    "    suffix_regex = re.compile(r'\\s\\(\\d+\\)')\n",
    "\n",
    "    total_moved = 0\n",
    "    total_skipped = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    print(\"--- Scanning for files with ' (n)' suffixes to move ---\")\n",
    "\n",
    "    # --- 2. Read file list and move files ---\n",
    "    try:\n",
    "        with open(file_list_path, 'r') as f:\n",
    "            for line in f:\n",
    "                # Get the filename from the line.\n",
    "                # We'll split the line and take the last part.\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 2: # Skip 'total' or blank lines\n",
    "                    continue\n",
    "\n",
    "                # The filename is everything from the 8th part on\n",
    "                # (after the time '19:53')\n",
    "                filename = \" \".join(parts[8:])\n",
    "\n",
    "                if not filename or filename == \"mddupes\":\n",
    "                    continue\n",
    "\n",
    "                # Check if the filename has the ' (n)' pattern\n",
    "                name_part, extension = os.path.splitext(filename)\n",
    "                if suffix_regex.search(name_part):\n",
    "                    # This is a file to move\n",
    "                    try:\n",
    "                        source_path = filename\n",
    "                        dest_path = os.path.join(dest_folder, filename)\n",
    "\n",
    "                        if os.path.exists(source_path):\n",
    "                            shutil.move(source_path, dest_path)\n",
    "                            print(f\"Moved:   {filename}\")\n",
    "                            total_moved += 1\n",
    "                        else:\n",
    "                            print(f\"Skipped: {filename} (File not found)\")\n",
    "                            total_skipped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error moving {filename}: {e}\")\n",
    "                        total_errors += 1\n",
    "                else:\n",
    "                    # This is an original file, we skip it\n",
    "                    total_skipped += 1\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file list '{file_list_path}' was not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred reading the list file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Files Moved:   {total_moved}\")\n",
    "    print(f\"Files Skipped: {total_skipped} (Originals or not found)\")\n",
    "    print(f\"Errors:        {total_errors}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the new file list\n",
    "    move_duplicates_by_filename('nnnn.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9p49dwqYWBe"
   },
   "source": [
    "Reads an 'ls -ls' list, confirms true duplicates by hash,\n",
    "    keeps the first file (alphabetically), and moves the rest\n",
    "    to the destination folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pzrxr5g8Tgvg"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Calculates the SHA-256 hash of a file.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        # File might have been moved by a previous step\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def verify_and_move_duplicates(file_list_path, dest_folder=\"mddupes\"):\n",
    "    \"\"\"\n",
    "    Reads an 'ls -ls' list, confirms true duplicates by hash,\n",
    "    keeps the first file (alphabetically), and moves the rest\n",
    "    to the destination folder.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Setup ---\n",
    "    try:\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        print(f\"Using destination folder: '{dest_folder}/'\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {dest_folder}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Regex to parse 'ls -ls' output\n",
    "    line_regex = re.compile(\n",
    "        r'^\\s*(\\d+)\\s+([\\-drwx]+)\\s+(\\d+)\\s+([\\w\\d]+)\\s+([\\w\\d]+)\\s+(\\d+)\\s+([\\w\\s\\d\\:]+)\\s+(.*)$'\n",
    "    )\n",
    "\n",
    "    size_groups = defaultdict(list)\n",
    "    zero_byte_files = []\n",
    "\n",
    "    total_moved = 0\n",
    "    total_kept = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    # --- 2. Parse the file list and group by size ---\n",
    "    try:\n",
    "        with open(file_list_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('total'):\n",
    "                    continue\n",
    "\n",
    "                match = line_regex.match(line)\n",
    "                if match:\n",
    "                    try:\n",
    "                        permissions = match.group(2)\n",
    "                        byte_size = int(match.group(6))\n",
    "                        filename = match.group(8)\n",
    "\n",
    "                        if permissions.startswith('d'):\n",
    "                            continue # Skip directories\n",
    "                        elif byte_size == 0:\n",
    "                            zero_byte_files.append(filename)\n",
    "                        else:\n",
    "                            size_groups[byte_size].append(filename)\n",
    "                    except:\n",
    "                        continue # Ignore malformed lines\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_list_path}' was not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"--- Verifying file content and moving duplicates ---\")\n",
    "\n",
    "    # --- 3. Verify by hash and move duplicates ---\n",
    "    sorted_groups = sorted(size_groups.items(), key=lambda item: item[0])\n",
    "\n",
    "    for byte_size, filenames in sorted_groups:\n",
    "        # We only need to hash groups with potential dupes\n",
    "        if len(filenames) < 2:\n",
    "            if filenames:\n",
    "                total_kept += 1\n",
    "            continue\n",
    "\n",
    "        # Group files in this size-group by their content hash\n",
    "        hash_groups = defaultdict(list)\n",
    "        for name in filenames:\n",
    "            file_hash = get_file_hash(name)\n",
    "            if file_hash:\n",
    "                hash_groups[file_hash].append(name)\n",
    "\n",
    "        # Now process the hash groups\n",
    "        for h, files in hash_groups.items():\n",
    "            if len(files) > 1:\n",
    "                # This is a TRUE duplicate group.\n",
    "                # Sort alphabetically to pick the \"original\"\n",
    "                files.sort()\n",
    "\n",
    "                file_to_keep = files[0]\n",
    "                dupes_to_move = files[1:]\n",
    "\n",
    "                print(f\"\\n--- Group (Hash: {h[:10]}...) ---\")\n",
    "                print(f\"  Keeping: {file_to_keep}\")\n",
    "                total_kept += 1\n",
    "\n",
    "                for dupe in dupes_to_move:\n",
    "                    try:\n",
    "                        source_path = dupe\n",
    "                        dest_path = os.path.join(dest_folder, dupe)\n",
    "\n",
    "                        # Check if file still exists before moving\n",
    "                        if os.path.exists(source_path):\n",
    "                            shutil.move(source_path, dest_path)\n",
    "                            print(f\"  Moved:   {dupe}\")\n",
    "                            total_moved += 1\n",
    "                        else:\n",
    "                            print(f\"  Skipped: {dupe} (already moved or missing)\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error moving {dupe}: {e}\")\n",
    "                        total_errors += 1\n",
    "\n",
    "            elif len(files) == 1:\n",
    "                # This was a false positive (same size, diff content)\n",
    "                # print(f\"Keeping {files[0]} (unique content)\")\n",
    "                total_kept += 1\n",
    "\n",
    "    # --- 4. Report on Zero-Byte Files ---\n",
    "    if zero_byte_files:\n",
    "        print(\"\\n--- Zero-Byte Files (Ignored) ---\")\n",
    "        for f in sorted(zero_byte_files):\n",
    "            print(f\"  - {f}\")\n",
    "        total_kept += len(zero_byte_files)\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Files Moved:   {total_moved}\")\n",
    "    print(f\"Files Kept:    {total_kept}\")\n",
    "    print(f\"Errors:        {total_errors}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This script reads the 'ls -ls' output, groups by size,\n",
    "    # calculates a hash to confirm true duplicates,\n",
    "    # keeps one file, and moves the others.\n",
    "    verify_and_move_duplicates('sizeaftersnake.txt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/empty.ipynb",
     "timestamp": 1763073822954
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
