{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "error",
     "timestamp": 1763073955623,
     "user": {
      "displayName": "steenspring",
      "userId": "01988066796823035936"
     },
     "user_tz": 300
    },
    "id": "jx7KVblaZO5a",
    "language": "python",
    "outputId": "8c2cdda9-cab5-4264-b2f1-ecf78b97225a"
   },
   "outputs": [],
   "source": [
    "# @title removes duplicate files in a directory, keeping the oldest version\n",
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"\n",
    "    Calculates the SHA-256 hash of a file.\n",
    "    Returns the hash or None if an error occurs.\n",
    "    \"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            # Read and update hash in chunks of 4K\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_and_move_duplicates():\n",
    "    \"\"\"\n",
    "    Finds duplicate files in the current directory and all subdirectories.\n",
    "    Keeps the file with the oldest modification time and moves the rest\n",
    "    to a 'dupeshit' folder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use os.getcwd() to run in the directory where the script is located\n",
    "    start_dir = os.getcwd()\n",
    "    dupes_dir = os.path.join(start_dir, \"dupeshit\")\n",
    "\n",
    "    # --- 1. Setup ---\n",
    "    try:\n",
    "        os.makedirs(dupes_dir, exist_ok=True)\n",
    "        print(f\"Duplicates will be moved to: {dupes_dir}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {dupes_dir}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Group files by size (fast pre-filter) ---\n",
    "    size_groups = defaultdict(list)\n",
    "    print(f\"Scanning files in {start_dir}...\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        # Don't scan the 'dupeshit' folder itself\n",
    "        if dirpath.startswith(dupes_dir):\n",
    "            continue\n",
    "\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Don't check the script file itself\n",
    "            if os.path.realpath(filepath) == os.path.realpath(__file__):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Use os.path.realpath to follow symlinks (shortcuts)\n",
    "                real_path = os.path.realpath(filepath)\n",
    "                if not os.path.isfile(real_path):\n",
    "                    continue\n",
    "\n",
    "                file_size = os.path.getsize(real_path)\n",
    "                if file_size > 0: # Ignore empty files\n",
    "                    size_groups[file_size].append(real_path)\n",
    "            except FileNotFoundError:\n",
    "                continue # Skip broken links\n",
    "\n",
    "    # --- 3. Group by content hash (slower, accurate check) ---\n",
    "    hash_groups = defaultdict(list)\n",
    "    total_files_checked = 0\n",
    "\n",
    "    print(\"Checking for duplicate content (this may take a while)...\")\n",
    "    for size, files in size_groups.items():\n",
    "        if len(files) < 2:\n",
    "            # Not a duplicate if only one file of this size\n",
    "            total_files_checked += 1\n",
    "            continue\n",
    "\n",
    "        # Only hash files that are potential duplicates\n",
    "        for filepath in files:\n",
    "            file_hash = get_file_hash(filepath)\n",
    "            if file_hash:\n",
    "                hash_groups[file_hash].append(filepath)\n",
    "            total_files_checked += 1\n",
    "\n",
    "    # --- 4. Find oldest and move the rest ---\n",
    "    total_moved = 0\n",
    "    total_kept = 0\n",
    "\n",
    "    print(\"\\n--- Processing Duplicates ---\")\n",
    "\n",
    "    for file_hash, files in hash_groups.items():\n",
    "        if len(files) < 2:\n",
    "            # This file had a unique hash\n",
    "            total_kept += 1\n",
    "            continue\n",
    "\n",
    "        # We have true duplicates. Now, find the oldest.\n",
    "        # Create a list of (modification_time, filepath) tuples\n",
    "        file_times = []\n",
    "        for filepath in files:\n",
    "            try:\n",
    "                mtime = os.path.getmtime(filepath)\n",
    "                file_times.append((mtime, filepath))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        # Sort by time (oldest first)\n",
    "        file_times.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Keep the first file (the oldest)\n",
    "        file_to_keep = file_times[0][1]\n",
    "        print(f\"\\nGroup (Hash: {file_hash[:10]}...):\")\n",
    "        print(f\"  Keeping (Oldest): {file_to_keep}\")\n",
    "        total_kept += 1\n",
    "\n",
    "        # Move all the others\n",
    "        dupes_to_move = file_times[1:]\n",
    "        for mtime, filepath in dupes_to_move:\n",
    "            try:\n",
    "                filename = os.path.basename(filepath)\n",
    "                dest_path = os.path.join(dupes_dir, filename)\n",
    "\n",
    "                # Handle name conflicts in the 'dupeshit' folder\n",
    "                counter = 1\n",
    "                while os.path.exists(dest_path):\n",
    "                    name, ext = os.path.splitext(filename)\n",
    "                    dest_path = os.path.join(dupes_dir, f\"{name}_{counter}{ext}\")\n",
    "                    counter += 1\n",
    "\n",
    "                shutil.move(filepath, dest_path)\n",
    "                print(f\"  Moved:            {filepath}\")\n",
    "                total_moved += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error moving {filepath}: {e}\")\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Total Files Scanned: {total_files_checked}\")\n",
    "    print(f\"Unique Files Kept:   {total_kept}\")\n",
    "    print(f\"Duplicates Moved:    {total_moved}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_move_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# @title Find and Move Duplicates --- IGNORE --- and keeps the oldest version moves the rest to a 'dupeshit' folder\n",
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"\n",
    "    Calculates the SHA-256 hash of a file.\n",
    "    Returns the hash or None if an error occurs.\n",
    "    \"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            # Read and update hash in chunks of 4K\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_and_move_duplicates():\n",
    "    \"\"\"\n",
    "    Finds duplicate files in the current directory and all subdirectories.\n",
    "    Keeps the file with the oldest modification time and moves the rest\n",
    "    to a 'dupeshit' folder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use os.getcwd() to run in the directory where the script is located\n",
    "    start_dir = os.getcwd()\n",
    "    dupes_dir = os.path.join(start_dir, \"dupeshit\")\n",
    "\n",
    "    # --- 1. Setup ---\n",
    "    try:\n",
    "        os.makedirs(dupes_dir, exist_ok=True)\n",
    "        print(f\"Duplicates will be moved to: {dupes_dir}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {dupes_dir}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Group files by size (fast pre-filter) ---\n",
    "    size_groups = defaultdict(list)\n",
    "    print(f\"Scanning files in {start_dir}...\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        # Don't scan the 'dupeshit' folder itself\n",
    "        if dirpath.startswith(dupes_dir):\n",
    "            continue\n",
    "\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Don't check the script file itself\n",
    "            if os.path.realpath(filepath) == os.path.realpath(__file__):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Use os.path.realpath to follow symlinks (shortcuts)\n",
    "                real_path = os.path.realpath(filepath)\n",
    "                if not os.path.isfile(real_path):\n",
    "                    continue\n",
    "\n",
    "                file_size = os.path.getsize(real_path)\n",
    "                if file_size > 0: # Ignore empty files\n",
    "                    size_groups[file_size].append(real_path)\n",
    "            except FileNotFoundError:\n",
    "                continue # Skip broken links\n",
    "\n",
    "    # --- 3. Group by content hash (slower, accurate check) ---\n",
    "    hash_groups = defaultdict(list)\n",
    "    total_files_checked = 0\n",
    "\n",
    "    print(\"Checking for duplicate content (this may take a while)...\")\n",
    "    for size, files in size_groups.items():\n",
    "        if len(files) < 2:\n",
    "            # Not a duplicate if only one file of this size\n",
    "            total_files_checked += 1\n",
    "            continue\n",
    "\n",
    "        # Only hash files that are potential duplicates\n",
    "        for filepath in files:\n",
    "            file_hash = get_file_hash(filepath)\n",
    "            if file_hash:\n",
    "                hash_groups[file_hash].append(filepath)\n",
    "            total_files_checked += 1\n",
    "\n",
    "    # --- 4. Find oldest and move the rest ---\n",
    "    total_moved = 0\n",
    "    total_kept = 0\n",
    "\n",
    "    print(\"\\n--- Processing Duplicates ---\")\n",
    "\n",
    "    for file_hash, files in hash_groups.items():\n",
    "        if len(files) < 2:\n",
    "            # This file had a unique hash\n",
    "            total_kept += 1\n",
    "            continue\n",
    "\n",
    "        # We have true duplicates. Now, find the oldest.\n",
    "        # Create a list of (modification_time, filepath) tuples\n",
    "        file_times = []\n",
    "        for filepath in files:\n",
    "            try:\n",
    "                mtime = os.path.getmtime(filepath)\n",
    "                file_times.append((mtime, filepath))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        # Sort by time (oldest first)\n",
    "        file_times.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Keep the first file (the oldest)\n",
    "        file_to_keep = file_times[0][1]\n",
    "        print(f\"\\nGroup (Hash: {file_hash[:10]}...):\")\n",
    "        print(f\"  Keeping (Oldest): {file_to_keep}\")\n",
    "        total_kept += 1\n",
    "\n",
    "        # Move all the others\n",
    "        dupes_to_move = file_times[1:]\n",
    "        for mtime, filepath in dupes_to_move:\n",
    "            try:\n",
    "                filename = os.path.basename(filepath)\n",
    "                dest_path = os.path.join(dupes_dir, filename)\n",
    "\n",
    "                # Handle name conflicts in the 'dupeshit' folder\n",
    "                counter = 1\n",
    "                while os.path.exists(dest_path):\n",
    "                    name, ext = os.path.splitext(filename)\n",
    "                    dest_path = os.path.join(dupes_dir, f\"{name}_{counter}{ext}\")\n",
    "                    counter += 1\n",
    "\n",
    "                shutil.move(filepath, dest_path)\n",
    "                print(f\"  Moved:            {filepath}\")\n",
    "                total_moved += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error moving {filepath}: {e}\")\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Total Files Scanned: {total_files_checked}\")\n",
    "    print(f\"Unique Files Kept:   {total_kept}\")\n",
    "    print(f\"Duplicates Moved:    {total_moved}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_move_duplicates()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
