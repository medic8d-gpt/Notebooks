{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": true,
    "id": "Pnj-1ol_uTGP"
   },
   "outputs": [],
   "source": [
    "# @title  Clay City Times (1901–1922) – High-Speed Downloader\n",
    "# Mode: Static link list + aria2c parallel bulk fetch\n",
    "# Target: /MyDrive/research_2/\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Install tools\n",
    "!pip install -q internetarchive tqdm\n",
    "!apt-get -qq install aria2\n",
    "\n",
    "import os, re, shutil\n",
    "from tqdm import tqdm\n",
    "import internetarchive as ia\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "BASE_DIR = '/content/drive/MyDrive/research_2'\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "LINK_LIST = '/content/clay_city_times_links.txt'\n",
    "YEARS = [str(y) for y in range(1901, 1923)]\n",
    "QUERY = 'creator:\"Clay City times\" AND (' + ' OR '.join(f'year:{y}' for y in YEARS) + ')'\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Build Static Download List (only once)\n",
    "# ==========================================================\n",
    "if not os.path.exists(LINK_LIST):\n",
    "    print(\"Generating item list from Archive.org (first run only)...\")\n",
    "    results = list(ia.search_items(QUERY))\n",
    "    with open(LINK_LIST, 'w') as f:\n",
    "        for r in tqdm(results, desc=\"Building link list\"):\n",
    "            ident = r['identifier']\n",
    "            f.write(f\"https://archive.org/download/{ident}/{ident}_djvu.txt\\n\")\n",
    "            f.write(f\"https://archive.org/download/{ident}/{ident}_meta.xml\\n\")\n",
    "    print(f\"List written to {LINK_LIST} ({len(results)} issues, 2 links each).\")\n",
    "else:\n",
    "    print(f\"Using existing link list: {LINK_LIST}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 4. High-speed Bulk Download\n",
    "# ==========================================================\n",
    "print(\"\\nStarting aria2c parallel download...\")\n",
    "!aria2c -x 8 -s 8 -i /content/clay_city_times_links.txt \\\n",
    "  -d \"/content/drive/MyDrive/research_2\" \\\n",
    "  --continue=true --auto-file-renaming=false --summary-interval=30\n",
    "\n",
    "# ==========================================================\n",
    "# 5. Organize files into per-issue folders\n",
    "# ==========================================================\n",
    "print(\"\\nOrganizing downloaded files...\")\n",
    "\n",
    "for f in tqdm(os.listdir(BASE_DIR), desc=\"Organizing\"):\n",
    "    if not os.path.isfile(os.path.join(BASE_DIR, f)):\n",
    "        continue\n",
    "    m = re.match(r'(claycitytimes|clay_city_times|claycity_times|claycity\\-times)?_?(\\d{4}[\\-_]\\d{2}[\\-_]\\d{2})?', f)\n",
    "    date = None\n",
    "    if m and m.group(2):\n",
    "        date = m.group(2).replace('_', '-')\n",
    "    else:\n",
    "        # fallback: parse year if available\n",
    "        yr_match = re.search(r'(19\\d{2})', f)\n",
    "        if yr_match:\n",
    "            date = yr_match.group(1)\n",
    "    folder = os.path.join(BASE_DIR, f\"clay_city_times_{date or 'unknown'}\")\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    try:\n",
    "        shutil.move(os.path.join(BASE_DIR, f), os.path.join(folder, f))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n✅ All issues downloaded and organized under MyDrive/research_2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "laxofACIde4z"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# SCRIPT C: (NEW) \"ENRICH & BUILD\" DATABASE SCRIPT\n",
    "#\n",
    "# This script REPLACES your old, simple database builder.\n",
    "# It uses Chunking (langchain) and Entity Extraction (spacy)\n",
    "# to build a much smarter database.\n",
    "#\n",
    "# Run this AFTER Script A (Converter) and Script B (Deduplicator).\n",
    "#\n",
    "# It will:\n",
    "# 1. Install langchain and spacy.\n",
    "# 2. Load both the Embedding model (for search) and an NER model (for metadata).\n",
    "# 3. Scan the clean 'Text_Files' directory.\n",
    "# 4. For each file, find its matching .xml file to get source metadata.\n",
    "# 5. Break the text into small, 500-character chunks.\n",
    "# 6. \"Read\" each chunk with spacy to find People, Places, and Orgs.\n",
    "# 7. Add each chunk to ChromaDB with all this new metadata.\n",
    "# 8. Move the final, enriched DB to Google Drive.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET # For reading the .xml files\n",
    "from google.colab import drive\n",
    "from tqdm.auto import tqdm # Progress bar\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Phase 1: Install Dependencies\n",
    "# ------------------------------------------------------------------\n",
    "print(\"--- Phase 1: Installing Dependencies ---\")\n",
    "!pip install chromadb sentence-transformers langchain spacy -q\n",
    "# Download the small, fast spacy model for Named Entity Recognition\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "print(\"Dependencies (Chroma, Langchain, Spacy) installed.\")\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import spacy\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Phase 2: Load Models\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Phase 2: Loading AI Models ---\")\n",
    "\n",
    "try:\n",
    "    print(\"Loading SentenceTransformer model (all-MiniLM-L6-v2) for embedding...\")\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"Embedding model loaded.\")\n",
    "\n",
    "    print(\"Loading Spacy NER model (en_core_web_sm) for entity extraction...\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"NER model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    raise e\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Phase 3: Configuration\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Phase 3: Configuring Paths ---\")\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    print(\"Google Drive mounted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting drive: {e}\")\n",
    "    raise e\n",
    "\n",
    "# --- CONFIGURED FOR CLAY CITY TIMES ---\n",
    "# This is the main output folder for your project\n",
    "MAIN_OUTPUT_DIRECTORY = '/content/drive/MyDrive/clay_city_times-2'\n",
    "\n",
    "# This is the ORIGINAL download folder where the .xml files are\n",
    "# We need this to read the source metadata\n",
    "XML_SOURCE_DIRECTORY = '/content/drive/MyDrive/clay_city_times-2'\n",
    "# -------------------------------------\n",
    "\n",
    "# This is the folder of CLEANED and DEDUPLICATED .txt files (from Script B)\n",
    "SOURCE_DIRECTORY = os.path.join(MAIN_OUTPUT_DIRECTORY, 'Text_Files')\n",
    "\n",
    "# --- DB Build Paths (Local first, then GDrive) ---\n",
    "LOCAL_DB_PATH = '/content/Clay_City_Enriched_DB'\n",
    "FINAL_DB_PATH_ON_DRIVE = os.path.join(MAIN_OUTPUT_DIRECTORY, 'Vector_Database_Enriched')\n",
    "COLLECTION_NAME = \"clay_city_archive_v2\" # New collection name\n",
    "\n",
    "# State file for the database log\n",
    "STATE_DIRECTORY = os.path.join(MAIN_OUTPUT_DIRECTORY, 'Script_State')\n",
    "DB_LOG_FILE = os.path.join(STATE_DIRECTORY, 'db_processed_files_v2.log')\n",
    "\n",
    "print(f\"Source (Input): {SOURCE_DIRECTORY}\")\n",
    "print(f\"XML Source (Metadata): {XML_SOURCE_DIRECTORY}\")\n",
    "print(f\"Local DB Build Path: {LOCAL_DB_PATH}\")\n",
    "print(f\"Final DB GDrive Path: {FINAL_DB_PATH_ON_DRIVE}\")\n",
    "\n",
    "os.makedirs(STATE_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(FINAL_DB_PATH_ON_DRIVE, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Phase 4: Helper Functions\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def parse_xml_metadata(txt_file_path):\n",
    "    \"\"\"\n",
    "    Finds the matching _meta.xml file for a .txt file and extracts\n",
    "    key metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the expected XML file path\n",
    "        # e.g., .../xt7b2r3nwv4x_djvu.txt -> .../xt7b2r3nwv4x_meta.xml\n",
    "        base_name = os.path.basename(txt_file_path).replace('_djvu.txt', '')\n",
    "        relative_dir = os.path.relpath(os.path.dirname(txt_file_path), SOURCE_DIRECTORY)\n",
    "        xml_file_path = os.path.join(XML_SOURCE_DIRECTORY, relative_dir, f\"{base_name}_meta.xml\")\n",
    "\n",
    "        if not os.path.exists(xml_file_path):\n",
    "            # No XML file found (this is normal for your other projects)\n",
    "            return {\"source_file\": os.path.basename(txt_file_path)}\n",
    "\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Define a namespace (often present in XML files)\n",
    "        ns = {'': root.tag.split('}')[0].strip('{') if '}' in root.tag else ''}\n",
    "        def find_tag(tag):\n",
    "            el = root.find(f\"{{}}{tag}\", ns)\n",
    "            return el.text if el is not None else \"unknown\"\n",
    "\n",
    "        metadata = {\n",
    "            \"source_file\": os.path.basename(txt_file_path),\n",
    "            \"date\": find_tag(\"date\"),\n",
    "            \"publisher\": find_tag(\"publisher\"),\n",
    "            \"county\": find_tag(\"county\"),\n",
    "            \"title\": find_tag(\"title\")\n",
    "        }\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"  WARN: Could not parse XML for {txt_file_path}. {e}\")\n",
    "        return {\"source_file\": os.path.basename(txt_file_path)}\n",
    "\n",
    "def extract_entities(chunk_text):\n",
    "    \"\"\"\n",
    "    Uses Spacy to extract People, Places, and Organizations from a chunk.\n",
    "    \"\"\"\n",
    "    # We use lists to avoid duplicate entries\n",
    "    people = set()\n",
    "    places = set()\n",
    "    orgs = set()\n",
    "\n",
    "    doc = nlp(chunk_text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.add(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]: # GPE=Geo-Political, LOC=Location\n",
    "            places.add(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            orgs.add(ent.text)\n",
    "\n",
    "    # Return as comma-separated strings (cleaner for ChromaDB metadata)\n",
    "    return {\n",
    "        \"people\": \", \".join(people),\n",
    "        \"places\": \", \".join(places),\n",
    "        \"orgs\": \", \".join(orgs)\n",
    "    }\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Size of each chunk\n",
    "    chunk_overlap=50 # Overlap to keep context\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Phase 5: Scan, Chunk, Enrich, and Build Database\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Phase 5: Building Enriched Vector Database ---\")\n",
    "\n",
    "print(\"Loading processing state...\")\n",
    "processed_files = set()\n",
    "try:\n",
    "    if os.path.exists(DB_LOG_FILE):\n",
    "        with open(DB_LOG_FILE, 'r', encoding='utf-8') as f:\n",
    "            processed_files = set(f.read().splitlines())\n",
    "        print(f\"Loaded {len(processed_files)} *file* (not chunk) records.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load DB state, starting fresh. Error: {e}\")\n",
    "\n",
    "client = chromadb.PersistentClient(path=LOCAL_DB_PATH)\n",
    "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "print(f\"DB client running. Collection '{COLLECTION_NAME}' has {collection.count()} *chunks*.\")\n",
    "\n",
    "start_time = time.time()\n",
    "new_files_processed = 0\n",
    "new_chunks_added = 0\n",
    "batch_documents, batch_metadatas, batch_ids = [], [], []\n",
    "BATCH_SIZE = 100 # We can use a larger batch size now\n",
    "\n",
    "try:\n",
    "    # First, get a list of all files that need processing\n",
    "    files_to_process = []\n",
    "    for dirpath, dirnames, filenames in os.walk(SOURCE_DIRECTORY):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                # --- RESUMABILITY ---\n",
    "                if file_path not in processed_files:\n",
    "                    files_to_process.append(file_path)\n",
    "\n",
    "    print(f\"Found {len(files_to_process)} new files to chunk and embed.\")\n",
    "\n",
    "    # Now, process them with a progress bar\n",
    "    with open(DB_LOG_FILE, 'a', encoding='utf-8') as f_log:\n",
    "        for file_path in tqdm(files_to_process, desc=\"Processing files\"):\n",
    "            try:\n",
    "                # 1. Get Source Metadata (from XML)\n",
    "                source_metadata = parse_xml_metadata(file_path)\n",
    "\n",
    "                # 2. Read and Chunk Text\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                if not content.strip():\n",
    "                    f_log.write(file_path + '\\n') # Log as \"processed\"\n",
    "                    continue\n",
    "\n",
    "                chunks = text_splitter.split_text(content)\n",
    "\n",
    "                # 3. Process each chunk\n",
    "                for i, chunk_text in enumerate(chunks):\n",
    "\n",
    "                    # 4. Extract Entities (from Spacy)\n",
    "                    entity_metadata = extract_entities(chunk_text)\n",
    "\n",
    "                    # 5. Combine all metadata\n",
    "                    full_metadata = {**source_metadata, **entity_metadata, \"chunk_num\": i}\n",
    "\n",
    "                    # 6. Create a unique ID for this specific chunk\n",
    "                    chunk_id = f\"{file_path}_{i}\"\n",
    "\n",
    "                    # Add to our batch\n",
    "                    batch_documents.append(chunk_text)\n",
    "                    batch_metadatas.append(full_metadata)\n",
    "                    batch_ids.append(chunk_id)\n",
    "                    new_chunks_added += 1\n",
    "\n",
    "                    # 7. Add to DB when batch is full\n",
    "                    if len(batch_ids) >= BATCH_SIZE:\n",
    "                        embeddings = embedding_model.encode(batch_documents).tolist()\n",
    "                        collection.add(\n",
    "                            embeddings=embeddings,\n",
    "                            documents=batch_documents,\n",
    "                            metadatas=batch_metadatas,\n",
    "                            ids=batch_ids\n",
    "                        )\n",
    "                        batch_documents, batch_metadatas, batch_ids = [], [], []\n",
    "\n",
    "                # After all chunks for a file are processed, log the file\n",
    "                f_log.write(file_path + '\\n')\n",
    "                new_files_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  ERROR: Failed to process file {file_path}. {e}\")\n",
    "\n",
    "    # Add the final batch\n",
    "    if batch_ids:\n",
    "        print(f\"\\n... Adding final batch of {len(batch_ids)} chunks ...\")\n",
    "        embeddings = embedding_model.encode(batch_documents).tolist()\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An error occurred during embedding: {e} ---\")\n",
    "finally:\n",
    "    print(\"\\n--- Phase 5 Embedding finished ---\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds.\")\n",
    "    print(f\"Total new files processed: {new_files_processed}\")\n",
    "    print(f\"Total new CHUNKS added to DB: {new_chunks_added}\")\n",
    "    print(f\"\\nDatabase now contains {collection.count()} total chunks.\")\n",
    "    print(f\"Database is built locally at: {LOCAL_DB_PATH}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Phase 6: Test Query (Now with metadata filtering)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Phase 6: Running a Test Query ---\")\n",
    "\n",
    "if collection.count() > 0:\n",
    "    # We'll do an advanced query\n",
    "    query_text = \"Who died in Stanton?\"\n",
    "\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "\n",
    "    # 1. Create the embedding for the query\n",
    "    query_embedding = embedding_model.encode([query_text]).tolist()\n",
    "\n",
    "    # 2. Query the collection WITH a metadata filter\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=3,\n",
    "        # This is the new, powerful part:\n",
    "        where={\"places\": {\"$like\": \"%Stanton%\"}},\n",
    "        include=['documents', 'metadatas', 'distances']\n",
    "    )\n",
    "\n",
    "    print(\"--- Top 3 Results (Filtered for 'Stanton') ---\")\n",
    "    if 'ids' in results and results['ids'][0]:\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            distance = results['distances'][0][i]\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            document = results['documents'][0][i][:350] + \"...\"\n",
    "\n",
    "            print(f\"\\nResult {i+1} (Distance: {distance:.4f})\")\n",
    "            print(f\"  Source: {metadata.get('source_file', 'N/A')} (Chunk {metadata.get('chunk_num', 'N/A')})\")\n",
    "            print(f\"  Date: {metadata.get('date', 'N/A')}\")\n",
    "            print(f\"  People: {metadata.get('people', 'N/A')}\")\n",
    "            print(f\"  Places: {metadata.get('places', 'N/A')}\")\n",
    "            print(f\"  Content: {document}\")\n",
    "    else:\n",
    "        print(\"No results found matching both the query and the filter.\")\n",
    "else:\n",
    "    print(\"Database is empty. No query performed.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Phase 7: Copy Finished DB to Google Drive\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Phase 7: Moving Database to Google Drive ---\")\n",
    "print(f\"Moving local DB from: {LOCAL_DB_PATH}\")\n",
    "print(f\"                to: {FINAL_DB_PATH_ON_DRIVE}\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(FINAL_DB_PATH_ON_DRIVE):\n",
    "        print(\"Removing empty placeholder folder from Google Drive...\")\n",
    "        shutil.rmtree(FINAL_DB_PATH_ON_DRIVE)\n",
    "\n",
    "    !mv {LOCAL_DB_PATH} {FINAL_DB_PATH_ON_DRIVE}\n",
    "\n",
    "    print(\"\\n--- Move complete! ---\")\n",
    "    print(f\"Your persistent, ENRICHED database is now saved at: {FINAL_DB_PATH_ON_DRIVE}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERROR: Could not move database to Google Drive ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"Your database is still safe on the local Colab disk at: {LOCAL_DB_PATH}\")\n",
    "\n",
    "print(\"\\n--- 'ENRICH & BUILD' SCRIPT FINISHED ---\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/empty.ipynb",
     "timestamp": 1762878162015
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
