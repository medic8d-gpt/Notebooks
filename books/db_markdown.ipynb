{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "# @title returns tables and rows\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your database file (update if it's in Google Drive)\n",
        "db_path = \"articles.db\"\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# List all tables\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = [t[0] for t in cursor.fetchall()]\n",
        "\n",
        "print(\"Found tables:\", tables)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Loop through and show 5 rows per table\n",
        "for table in tables:\n",
        "    print(f\"\\n=== TABLE: {table} ===\")\n",
        "    try:\n",
        "        df = pd.read_sql_query(f\"SELECT * FROM {table} LIMIT 5;\", conn)\n",
        "        display(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading table {table}: {e}\")\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "x_a3TuHWAZsm"
      },
      "outputs": [],
      "source": [
        "# @title takes db to markdownfiles\n",
        "import os, sqlite3, pandas as pd, time, signal, sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === CONFIG ===\n",
        "DB_PATH = \"articles.db\"  # Path to your DB\n",
        "OUTPUT_DIR = \"/content/Markdown_from_articles.db\"\n",
        "TABLE = \"clean_articles\"\n",
        "HEARTBEAT_INTERVAL = 30  # seconds\n",
        "\n",
        "# === SETUP ===\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"üìÇ Output folder: {OUTPUT_DIR}\")\n",
        "\n",
        "# Track graceful exit\n",
        "running = True\n",
        "def handle_sigint(sig, frame):\n",
        "    global running\n",
        "    print(\"\\n‚ö†Ô∏è Graceful stop requested... finishing current row then exiting.\")\n",
        "    running = False\n",
        "signal.signal(signal.SIGINT, handle_sigint)\n",
        "\n",
        "# Connect DB\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Count total rows\n",
        "cursor.execute(f\"SELECT COUNT(*) FROM {TABLE};\")\n",
        "total_rows = cursor.fetchone()[0]\n",
        "print(f\"üßæ Total rows found: {total_rows}\\n\")\n",
        "\n",
        "# Heartbeat thread\n",
        "import threading\n",
        "def heartbeat():\n",
        "    while running:\n",
        "        print(f\"[Heartbeat] Still running... {time.strftime('%H:%M:%S')}\")\n",
        "        time.sleep(HEARTBEAT_INTERVAL)\n",
        "threading.Thread(target=heartbeat, daemon=True).start()\n",
        "\n",
        "# Load rows in chunks\n",
        "CHUNK_SIZE = 100\n",
        "offset = 0\n",
        "exported_count = len(os.listdir(OUTPUT_DIR))\n",
        "print(f\"üîÑ Resuming from file #{exported_count} if possible.\\n\")\n",
        "\n",
        "try:\n",
        "    with tqdm(total=total_rows, desc=\"Exporting Articles\", initial=exported_count) as pbar:\n",
        "        while running:\n",
        "            df = pd.read_sql_query(\n",
        "                f\"SELECT * FROM {TABLE} LIMIT {CHUNK_SIZE} OFFSET {offset};\",\n",
        "                conn\n",
        "            )\n",
        "            if df.empty:\n",
        "                print(\"\\n‚úÖ Finished all available rows.\")\n",
        "                break\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                if not running:\n",
        "                    break\n",
        "\n",
        "                article_id = str(row.get(\"id\", f\"offset_{offset}\"))\n",
        "                title = str(row.get(\"title\", f\"Article_{article_id}\"))\n",
        "                url = str(row.get(\"url\", \"\"))\n",
        "                ts = str(row.get(\"ts\", \"\"))\n",
        "                content = str(row.get(\"content\", row.get(\"text\", \"\")))\n",
        "\n",
        "                safe_title = \"\".join(c for c in title if c.isalnum() or c in (\" \", \"_\", \"-\")).strip()\n",
        "                filename = os.path.join(\n",
        "                    OUTPUT_DIR,\n",
        "                    f\"{article_id.zfill(6)}_{safe_title[:60].replace(' ', '_')}.md\"\n",
        "                )\n",
        "\n",
        "                if os.path.exists(filename):\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                md = f\"\"\"---\n",
        "id: {article_id}\n",
        "title: \"{title}\"\n",
        "url: {url}\n",
        "timestamp: {ts}\n",
        "---\n",
        "\n",
        "# {title}\n",
        "\n",
        "{content}\n",
        "\"\"\"\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(md)\n",
        "\n",
        "                pbar.update(1)\n",
        "                offset += 1\n",
        "\n",
        "            if df.shape[0] < CHUNK_SIZE:\n",
        "                break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "finally:\n",
        "    conn.close()\n",
        "    print(f\"\\nüíæ Export stopped. {len(os.listdir(OUTPUT_DIR))} files currently in folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title #1 install required packages\n",
        "!pip install openai chromadb faiss-cpu tqdm google-auth google-colab\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title #2  Load OpenAI API Key from Colab Secrets\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Replace 'OPENAI_API_KEY' with the name of your stored secret\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "print(\"‚úÖ OpenAI API key loaded from Colab secrets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title  #3 Markdown Embedding App for Google Colab\n",
        "# ---------------------------------------\n",
        "# Purpose: Convert Markdown files into a vector database stored on Google Drive.\n",
        "# This script is designed for Colab Free Tier: efficient, fault-tolerant, resumable, and fully logged.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Dependencies\n",
        "# Install necessary packages and mount Google Drive.\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import os, json, time, uuid, datetime\n",
        "from tqdm import tqdm\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Configuration Input\n",
        "# Ask the user for input and setup paths.\n",
        "\n",
        "input_dir = input(\"Enter the full path to your folder of Markdown files: \").strip()\n",
        "output_root = \"/content/drive/MyDrive/vector_dbs/\"\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "version_id = str(uuid.uuid4())\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "output_dir = os.path.join(output_root, f\"session_{timestamp}\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "heartbeat_interval = 30  # seconds\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. File Discovery\n",
        "# Scan for `.md` files and prepare processing queue.\n",
        "\n",
        "files_to_process = []\n",
        "for root, _, files in os.walk(input_dir):\n",
        "    for f in files:\n",
        "        if f.endswith('.md'):\n",
        "            files_to_process.append(os.path.join(root, f))\n",
        "\n",
        "print(f\"Found {len(files_to_process)} Markdown files to process.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Heartbeat Monitor\n",
        "# Prints periodic status updates while processing.\n",
        "\n",
        "import threading\n",
        "\n",
        "def heartbeat():\n",
        "    while True:\n",
        "        print(f\"[Heartbeat] App is active... {datetime.datetime.now().strftime('%H:%M:%S')}\")\n",
        "        time.sleep(heartbeat_interval)\n",
        "\n",
        "threading.Thread(target=heartbeat, daemon=True).start()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Embedding Engine Setup\n",
        "# Initialize the embedding model and Chroma vector database.\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "vector_db = Chroma(collection_name=\"markdown_knowledge\", embedding_function=embedding_model, persist_directory=output_dir)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Processing Loop\n",
        "# Read, embed, and store each Markdown file.\n",
        "\n",
        "success_count = 0\n",
        "fail_count = 0\n",
        "report_log = []\n",
        "\n",
        "for file_path in tqdm(files_to_process, desc=\"Embedding Markdown Files\"):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        if not content.strip():\n",
        "            raise ValueError(\"File empty.\")\n",
        "\n",
        "        metadata = {\n",
        "            \"file\": os.path.basename(file_path),\n",
        "            \"path\": file_path,\n",
        "            \"timestamp\": timestamp,\n",
        "        }\n",
        "\n",
        "        vector_db.add_texts([content], metadatas=[metadata])\n",
        "        success_count += 1\n",
        "        report_log.append({\"file\": file_path, \"status\": \"Success\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        fail_count += 1\n",
        "        report_log.append({\"file\": file_path, \"status\": \"Failed\", \"error\": str(e)})\n",
        "        continue\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Save and Persist Database\n",
        "# Commit the database to disk and ensure all data is flushed.\n",
        "\n",
        "vector_db.persist()\n",
        "\n",
        "with open(os.path.join(output_dir, \"report.json\"), 'w') as f:\n",
        "    json.dump(report_log, f, indent=2)\n",
        "\n",
        "print(f\"\\nEmbedding completed: {success_count} success, {fail_count} failed.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Version Tracking\n",
        "# Create or update version history file with details for GPT reference.\n",
        "\n",
        "version_entry = {\n",
        "    \"version_id\": version_id,\n",
        "    \"created\": timestamp,\n",
        "    \"files_embedded\": success_count,\n",
        "    \"failures\": fail_count,\n",
        "    \"source_dir\": input_dir,\n",
        "    \"db_path\": output_dir\n",
        "}\n",
        "\n",
        "version_log_path = os.path.join(output_root, \"version_history.json\")\n",
        "\n",
        "if os.path.exists(version_log_path):\n",
        "    with open(version_log_path, 'r') as f:\n",
        "        history = json.load(f)\n",
        "else:\n",
        "    history = []\n",
        "\n",
        "history.append(version_entry)\n",
        "with open(version_log_path, 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "print(f\"Version history updated. ID: {version_id}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Final Report\n",
        "# Generate HTML summary like _report.html for review.\n",
        "\n",
        "html_report = f\"\"\"\n",
        "<html><head><title>Embedding Report</title></head><body>\n",
        "<h1>Markdown Embedding Report</h1>\n",
        "<p><strong>Session:</strong> {timestamp}</p>\n",
        "<p><strong>Version ID:</strong> {version_id}</p>\n",
        "<p><strong>Success:</strong> {success_count}</p>\n",
        "<p><strong>Failures:</strong> {fail_count}</p>\n",
        "<p><strong>Database Path:</strong> {output_dir}</p>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(output_dir, \"embedding_report.html\"), 'w') as f:\n",
        "    f.write(html_report)\n",
        "\n",
        "print(f\"Report saved to {output_dir}/embedding_report.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 4. (Optional) Load Existing Database Session\n",
        "#\n",
        "# Use this cell INSTEAD of running the main embedding process (cell #5)\n",
        "# if you only want to load an old database (e.g., to export it to JSON).\n",
        "#\n",
        "# You must run the setup cells (1-4) first to install packages and load your API key.\n",
        "\n",
        "import os\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# --- 1. Initialize Embedding Model ---\n",
        "# We must initialize the *same* embedding model that was used to create the DB.\n",
        "try:\n",
        "    if 'embedding_model' not in locals():\n",
        "         print(\"Initializing embedding model (text-embedding-3-large)...\")\n",
        "         embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "    else:\n",
        "         print(\"Embedding model already loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing embedding model: {e}\")\n",
        "    print(\"Please ensure your OPENAI_API_KEY is set in Cell 4.\")\n",
        "\n",
        "# --- 2. Get Path from User ---\n",
        "print(\"\\n--- Load Existing Database ---\")\n",
        "print(\"Provide the full path to the *existing* session folder you want to load.\")\n",
        "print(\"Example: /content/drive/MyDrive/vector_dbs/session_2025-10-31_12-00-00\")\n",
        "existing_db_path = input(\"Enter path to session folder: \").strip()\n",
        "\n",
        "# --- 3. Load the Database ---\n",
        "if 'embedding_model' in locals() and os.path.isdir(existing_db_path):\n",
        "    try:\n",
        "        print(f\"\\nLoading database from: {existing_db_path}\")\n",
        "\n",
        "        # This points vector_db to your *existing* persisted database\n",
        "        vector_db = Chroma(\n",
        "            collection_name=\"markdown_knowledge\",\n",
        "            embedding_function=embedding_model,\n",
        "            persist_directory=existing_db_path\n",
        "        )\n",
        "\n",
        "        # This global variable is used by the JSON export cell (Cell 10)\n",
        "        # to know where to save the new JSON files.\n",
        "        output_dir = existing_db_path\n",
        "\n",
        "        print(f\"‚úÖ Successfully loaded {vector_db._collection.count()} items.\")\n",
        "        print(f\"The 'vector_db' variable is now set to this database.\")\n",
        "        print(\"You can now run the JSON export cell (Cell 10).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error loading database: {e}\")\n",
        "        print(\"Please check the path and ensure the folder contains a valid Chroma database.\")\n",
        "elif not os.path.isdir(existing_db_path):\n",
        "    print(f\"\\n‚ùå Error: Directory not found at path: {existing_db_path}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Error: Embedding model not initialized. Run cell 4.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 5. (Optional) Export Database to JSON Chunks\n",
        "# This cell retrieves all embedded data from the Chroma database\n",
        "# and exports it into a series of JSON files, each limited to ~20MB.\n",
        "#\n",
        "# Each JSON file will contain an \"export_info\" block with the date\n",
        "# and a unique version timestamp for this specific conversion.\n",
        "\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "MAX_CHUNK_SIZE_MB = 15.5  # Set to slightly less than 20MB for safety\n",
        "MAX_BYTES = MAX_CHUNK_SIZE_MB * 1024 * 1024\n",
        "json_export_dir = os.path.join(output_dir, \"json_export\")\n",
        "os.makedirs(json_export_dir, exist_ok=True)\n",
        "# ---------------------\n",
        "\n",
        "print(f\"Retrieving all data from vector database...\")\n",
        "try:\n",
        "    # Get all documents, metadatas, and their IDs\n",
        "    results = vector_db.get(include=[\"metadatas\", \"documents\"])\n",
        "\n",
        "    all_data = []\n",
        "    for i in range(len(results['ids'])):\n",
        "        all_data.append({\n",
        "            \"id\": results['ids'][i],\n",
        "            \"document\": results['documents'][i],\n",
        "            \"metadata\": results['metadatas'][i]\n",
        "        })\n",
        "\n",
        "    # --- NEW: Generate metadata for this export job ---\n",
        "    total_items_in_db = len(all_data)\n",
        "\n",
        "    # This timestamp will be the unique \"version #\" for this conversion\n",
        "    export_version = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S_UTC\")\n",
        "    export_date_utc = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    print(f\"Retrieved {total_items_in_db} items. Now chunking...\")\n",
        "    print(f\"This export's unique version: {export_version}\")\n",
        "\n",
        "    current_chunk_data = [] # This list will hold the items for the current chunk\n",
        "    chunk_index = 1\n",
        "\n",
        "    pbar = tqdm(all_data, desc=\"Exporting to JSON chunks\")\n",
        "    for item in pbar:\n",
        "        # Add item and check new total size\n",
        "        current_chunk_data.append(item)\n",
        "\n",
        "        # --- MODIFIED: Create the full JSON object for size checking ---\n",
        "        # This is the new structure that will be written to the file\n",
        "        json_to_write = {\n",
        "            \"export_info\": {\n",
        "                \"export_version\": export_version,\n",
        "                \"export_date_utc\": export_date_utc,\n",
        "                \"total_items_in_db\": total_items_in_db,\n",
        "                \"chunk_index\": chunk_index,\n",
        "                \"items_in_this_chunk\": len(current_chunk_data)\n",
        "            },\n",
        "            \"chunk_data\": current_chunk_data # The list of items\n",
        "        }\n",
        "\n",
        "        current_json_string = json.dumps(json_to_write)\n",
        "        current_size_bytes = len(current_json_string.encode('utf-8'))\n",
        "\n",
        "        # If chunk exceeds max size, write the *previous* state\n",
        "        if current_size_bytes > MAX_BYTES and len(current_chunk_data) > 1:\n",
        "            # Pop the last item that caused the overflow\n",
        "            item_to_move = current_chunk_data.pop()\n",
        "\n",
        "            # --- MODIFIED: Create the final object *without* the overflow item ---\n",
        "            final_chunk_data_list = list(current_chunk_data) # The list *before* the last item\n",
        "            json_to_write = {\n",
        "                \"export_info\": {\n",
        "                    \"export_version\": export_version,\n",
        "                    \"export_date_utc\": export_date_utc,\n",
        "                    \"total_items_in_db\": total_items_in_db,\n",
        "                    \"chunk_index\": chunk_index,\n",
        "                    \"items_in_this_chunk\": len(final_chunk_data_list)\n",
        "                },\n",
        "                \"chunk_data\": final_chunk_data_list\n",
        "            }\n",
        "\n",
        "            # Write the chunk\n",
        "            json_export_path = os.path.join(json_export_dir, f\"data_chunk_{chunk_index:03d}.json\")\n",
        "            with open(json_export_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(json_to_write, f, indent=2)\n",
        "\n",
        "            pbar.set_description(f\"Wrote chunk {chunk_index}\")\n",
        "\n",
        "            # Start the new chunk with the item that didn't fit\n",
        "            current_chunk_data = [item_to_move]\n",
        "            chunk_index += 1\n",
        "\n",
        "        elif current_size_bytes > MAX_BYTES and len(current_chunk_data) == 1:\n",
        "            # This single item is larger than the max chunk size\n",
        "            # The 'json_to_write' object is already correct (with 1 item)\n",
        "            json_export_path = os.path.join(json_export_dir, f\"data_chunk_{chunk_index:03d}.json\")\n",
        "            with open(json_export_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(json_to_write, f, indent=2)\n",
        "\n",
        "            pbar.set_description(f\"Wrote large chunk {chunk_index}\")\n",
        "\n",
        "            # Reset for next loop\n",
        "            current_chunk_data = []\n",
        "            chunk_index += 1\n",
        "\n",
        "\n",
        "    # --- MODIFIED: Write any remaining data in the last chunk ---\n",
        "    if current_chunk_data:\n",
        "        json_to_write = {\n",
        "            \"export_info\": {\n",
        "                \"export_version\": export_version,\n",
        "                \"export_date_utc\": export_date_utc,\n",
        "                \"total_items_in_db\": total_items_in_db,\n",
        "                \"chunk_index\": chunk_index,\n",
        "                \"items_in_this_chunk\": len(current_chunk_data)\n",
        "            },\n",
        "            \"chunk_data\": current_chunk_data\n",
        "        }\n",
        "        json_export_path = os.path.join(json_export_dir, f\"data_chunk_{chunk_index:03d}.json\")\n",
        "        with open(json_export_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(json_to_write, f, indent=2)\n",
        "        pbar.set_description(f\"Wrote final chunk {chunk_index}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ JSON export complete. {chunk_index} file(s) saved in: {json_export_dir}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå An error occurred during JSON export: {e}\")\n",
        "    print(\"Ensure the variable 'vector_db' (from cell 5) or 'output_dir' (from cell 11) exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7bsu1vKBlKO"
      },
      "outputs": [],
      "source": [
        "!mv Markdown_from_articles.db ../Markdown_from_articles\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
