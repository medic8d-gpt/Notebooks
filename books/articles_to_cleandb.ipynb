{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7387,
     "status": "ok",
     "timestamp": 1763053776157,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "z5fuPkhYLcMN",
    "outputId": "7d127bf1-b678-4ba4-f0ee-b0601a49c0e6"
   },
   "outputs": [],
   "source": [
    "# @title  Install required packages\n",
    "!pip install readability-lxml beautifulsoup4 tqdm  chromadb fastembed lxml readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5619545,
     "status": "ok",
     "timestamp": 1763059735491,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "QOTn_kG1LjEN",
    "outputId": "95c0d181-7495-4e92-d6cc-d7f9f9b766a2"
   },
   "outputs": [],
   "source": [
    "# @title  turrnds headlines.json into a db   use this script to download articles in turbo mode\n",
    "import os\n",
    "import requests\n",
    "import sqlite3\n",
    "import threading\n",
    "import time\n",
    "from urllib.parse import urlparse, unquote\n",
    "from queue import Queue\n",
    "\n",
    "from readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------\n",
    "JSON_URL = \"https://api.lexkynews.com/headlines.json\"\n",
    "DOWNLOAD_DIR = \"/content/articles\"\n",
    "DB_PATH = \"/content/articles.db\"\n",
    "\n",
    "THREADS = 10               # <-- turbo mode\n",
    "HEARTBEAT_INTERVAL = 30    # seconds\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0\"\n",
    "}\n",
    "\n",
    "TERMINAL_STATUSES = {\"done\", \"blocked\", \"not_found\"}\n",
    "\n",
    "# Hard skip domain prefixes\n",
    "SKIP_DOMAINS = [\n",
    "    \"fox56news.com\",\n",
    "    \"www.fox56news.com\",\n",
    "]\n",
    "\n",
    "# Globals\n",
    "pbar = None\n",
    "queue = Queue()\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# HEARTBEAT\n",
    "# -----------------------------------------\n",
    "def heartbeat():\n",
    "    while True:\n",
    "        time.sleep(HEARTBEAT_INTERVAL)\n",
    "        if pbar:\n",
    "            pbar.write(f\"[♥] Alive... {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# DB FUNCTIONS\n",
    "# -----------------------------------------\n",
    "def init_db():\n",
    "    conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS articles (\n",
    "            url TEXT PRIMARY KEY,\n",
    "            filename TEXT,\n",
    "            status TEXT,\n",
    "            size INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "\n",
    "db_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def save_record(conn, url, filename, status, size):\n",
    "    with db_lock:\n",
    "        c = conn.cursor()\n",
    "        c.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO articles (url, filename, status, size)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (url, filename, status, size))\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# FILENAME NORMALIZATION\n",
    "# -----------------------------------------\n",
    "def url_to_safe_name(url):\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.rstrip(\"/\") or \"index\"\n",
    "    slug = unquote(path.split(\"/\")[-1]) or \"index\"\n",
    "    slug = slug.replace(\"?\", \"_\").replace(\"&\", \"_\").replace(\"=\", \"_\")\n",
    "    if not slug.lower().endswith(\".html\"):\n",
    "        slug += \".html\"\n",
    "    return slug\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# ARTICLE EXTRACTION\n",
    "# -----------------------------------------\n",
    "def extract_article(html):\n",
    "    doc = Document(html)\n",
    "    title = doc.short_title() or \"Untitled\"\n",
    "    cleaned = BeautifulSoup(doc.summary(), \"html.parser\")\n",
    "    for tag in cleaned([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    return f\"<h1>{title}</h1>\\n{str(cleaned)}\"\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# WORKER: downloads in parallel\n",
    "# -----------------------------------------\n",
    "def worker(conn, session):\n",
    "    while True:\n",
    "        url = queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "        try:\n",
    "            download_article(conn, session, url)\n",
    "        finally:\n",
    "            queue.task_done()\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# DOWNLOAD ONE ARTICLE (safe & resumable)\n",
    "# -----------------------------------------\n",
    "def download_article(conn, session, url):\n",
    "\n",
    "    # HARD BLOCK\n",
    "    domain = urlparse(url).netloc.lower()\n",
    "    for d in SKIP_DOMAINS:\n",
    "        if d in domain:\n",
    "            pbar.write(f\"[skip-domain] {url}\")\n",
    "            save_record(conn, url, None, \"blocked\", 0)\n",
    "            return\n",
    "\n",
    "    filename = url_to_safe_name(url)\n",
    "    filepath = os.path.join(DOWNLOAD_DIR, filename)\n",
    "\n",
    "    # Resume check\n",
    "    with db_lock:\n",
    "        c = conn.cursor()\n",
    "        c.execute(\"SELECT status FROM articles WHERE url = ?\", (url,))\n",
    "        row = c.fetchone()\n",
    "\n",
    "    if row and row[0] in TERMINAL_STATUSES and os.path.exists(filepath):\n",
    "        pbar.write(f\"[skip] {filename}\")\n",
    "        return\n",
    "\n",
    "    pbar.write(f\"[↓] {url}\")\n",
    "\n",
    "    try:\n",
    "        r = session.get(url, headers=HEADERS, timeout=20)\n",
    "\n",
    "        if r.status_code == 403:\n",
    "            pbar.write(f\"[403] {url}\")\n",
    "            save_record(conn, url, None, \"blocked\", 0)\n",
    "            return\n",
    "\n",
    "        if r.status_code == 404:\n",
    "            pbar.write(f\"[404] {url}\")\n",
    "            save_record(conn, url, None, \"not_found\", 0)\n",
    "            return\n",
    "\n",
    "        r.raise_for_status()\n",
    "\n",
    "        article_html = extract_article(r.text)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(article_html)\n",
    "\n",
    "        size = len(article_html.encode(\"utf-8\"))\n",
    "        save_record(conn, url, filename, \"done\", size)\n",
    "        pbar.write(f\"[ok] {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        pbar.write(f\"[ERR] {url}: {e}\")\n",
    "        save_record(conn, url, filename, \"error\", 0)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# MAIN\n",
    "# -----------------------------------------\n",
    "def run():\n",
    "    global pbar\n",
    "\n",
    "    os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "    conn = init_db()\n",
    "\n",
    "    # heartbeat\n",
    "    threading.Thread(target=heartbeat, daemon=True).start()\n",
    "\n",
    "    print(\"[*] Fetching JSON…\")\n",
    "    data = requests.get(JSON_URL).json()\n",
    "    items = data.get(\"items\", []) if isinstance(data, dict) else data\n",
    "\n",
    "    # normalize urls\n",
    "    urls = []\n",
    "    for item in items:\n",
    "        if isinstance(item, str):\n",
    "            url = item\n",
    "        elif isinstance(item, dict):\n",
    "            url = item.get(\"url\") or item.get(\"link\")\n",
    "        else:\n",
    "            continue\n",
    "        if url and url.startswith(\"http\"):\n",
    "            urls.append(url)\n",
    "\n",
    "    print(f\"[*] URLs total: {len(urls)}\")\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    # tqdm progress bar\n",
    "    pbar = tqdm(total=len(urls), unit=\"url\")\n",
    "\n",
    "    # create worker threads\n",
    "    threads = []\n",
    "    for _ in range(THREADS):\n",
    "        t = threading.Thread(target=worker, args=(conn, session))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # queue jobs\n",
    "    for url in urls:\n",
    "        queue.put(url)\n",
    "\n",
    "    # wait for all jobs\n",
    "    queue.join()\n",
    "\n",
    "    # stop workers\n",
    "    for _ in range(THREADS):\n",
    "        queue.put(None)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"[DONE] turbo mode complete.\")\n",
    "\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13234769,
     "status": "ok",
     "timestamp": 1763077978459,
     "user": {
      "displayName": "Ian Powell",
      "userId": "16967135907563078865"
     },
     "user_tz": 300
    },
    "id": "fpfST_yx02f_",
    "outputId": "bab86e1c-12b4-4073-d493-3db91da612fd"
   },
   "outputs": [],
   "source": [
    "# @title  turrnds headlines.json into a db\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "DB_PATH = \"articles.db\"\n",
    "JSON_URL = \"https://api.lexkynews.com/headlines.json\"\n",
    "USER_AGENT = \"Mozilla/5.0 (compatible; Downloader/1.0)\"\n",
    "\n",
    "BLOCK_DOMAINS = [\"fox56news.com\", \"www.fox56news.com\"]\n",
    "\n",
    "\n",
    "def init_db():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS articles (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT UNIQUE,\n",
    "            status TEXT,\n",
    "            html TEXT,\n",
    "            ts INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "\n",
    "def load_json():\n",
    "    print(\"[*] Fetching headlines.json…\")\n",
    "    r = requests.get(JSON_URL, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"items\", [])\n",
    "\n",
    "\n",
    "def get_resume_index(conn, urls):\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT url FROM articles ORDER BY id DESC LIMIT 1\")\n",
    "    row = c.fetchone()\n",
    "    if not row:\n",
    "        return 0\n",
    "\n",
    "    last_url = row[0]\n",
    "    print(f\"[resume] Last processed URL: {last_url}\")\n",
    "\n",
    "    if last_url in urls:\n",
    "        return urls.index(last_url) + 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def save(conn, url, status, html):\n",
    "    ts = int(time.time())\n",
    "    conn.execute(\n",
    "        \"INSERT OR REPLACE INTO articles (url, status, html, ts) VALUES (?, ?, ?, ?)\",\n",
    "        (url, status, html, ts)\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def download_url(url):\n",
    "    domain = urlparse(url).netloc.lower()\n",
    "    if \"fox56news.com\" in domain:\n",
    "        return \"blocked\", None\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=10)\n",
    "        if r.status_code == 403:\n",
    "            return \"blocked\", None\n",
    "        if r.status_code == 404:\n",
    "            return \"not_found\", None\n",
    "        r.raise_for_status()\n",
    "        return \"done\", r.text\n",
    "    except:\n",
    "        return \"error\", None\n",
    "\n",
    "\n",
    "def heartbeat(last, interval=10):\n",
    "    now = time.time()\n",
    "    if now - last >= interval:\n",
    "        print(f\"[♥] Alive... {time.strftime('%H:%M:%S')}\")\n",
    "        return now\n",
    "    return last\n",
    "\n",
    "\n",
    "def run():\n",
    "    conn = init_db()\n",
    "    items = load_json()\n",
    "\n",
    "    urls = [item[\"url\"] for item in items if \"url\" in item]\n",
    "    total = len(urls)\n",
    "\n",
    "    start = get_resume_index(conn, urls)\n",
    "    print(f\"[start] Resuming at index {start}/{total}\")\n",
    "\n",
    "    hb = time.time()\n",
    "    pbar = tqdm(total=total, initial=start, desc=\"Downloading\", ncols=100)\n",
    "\n",
    "    for i in range(start, total):\n",
    "        url = urls[i]\n",
    "\n",
    "        hb = heartbeat(hb)\n",
    "\n",
    "        status, html = download_url(url)\n",
    "        save(conn, url, status, html)\n",
    "\n",
    "        pbar.set_postfix({\"status\": status})\n",
    "        pbar.update(1)\n",
    "\n",
    "        # REMOVED sleep for max speed\n",
    "        # if you want polite mode, add: time.sleep(0.01)\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"[DONE] Completed all downloads.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "error",
     "timestamp": 1763078594091,
     "user": {
      "displayName": "Ian Powell",
      "userId": "16967135907563078865"
     },
     "user_tz": 300
    },
    "id": "-8MmoQTZpG4a",
    "outputId": "b50b8e16-5385-47de-c08c-4d6acf0dea3d"
   },
   "outputs": [],
   "source": [
    "# @title  clean downloaded articles into text\n",
    "import sqlite3\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "DB_PATH = \"articles.db\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "# INIT CLEAN TABLE\n",
    "# ---------------------------------------------\n",
    "def init_clean_table(conn):\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS clean_articles (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            url TEXT,\n",
    "            source TEXT,\n",
    "            title TEXT,\n",
    "            published TEXT,\n",
    "            text TEXT,\n",
    "            html_len INTEGER,\n",
    "            extracted_at INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# SIMPLE DATE PARSER (BEST-EFFORT)\n",
    "# ---------------------------------------------\n",
    "def extract_date(soup):\n",
    "    # Look for <time> element\n",
    "    t = soup.find(\"time\")\n",
    "    if t and t.get_text(strip=True):\n",
    "        return t.get_text(strip=True)\n",
    "\n",
    "    # Metadata\n",
    "    meta = soup.find(\"meta\", {\"property\":\"article:published_time\"})\n",
    "    if meta:\n",
    "        return meta.get(\"content\")\n",
    "\n",
    "    meta2 = soup.find(\"meta\", {\"name\":\"pubdate\"})\n",
    "    if meta2:\n",
    "        return meta2.get(\"content\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CLEAN TEXT VIA READABILITY FIRST\n",
    "# ---------------------------------------------\n",
    "def extract_content(html):\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        title = doc.title()\n",
    "        cleaned_html = doc.summary(html_partial=True)\n",
    "\n",
    "        soup = BeautifulSoup(cleaned_html, \"lxml\")\n",
    "\n",
    "        # text extraction\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "        return title, text\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# FALLBACK CLEANER (SOUP ONLY)\n",
    "# ---------------------------------------------\n",
    "def fallback_extract(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Drop scripts, styles\n",
    "    for t in soup([\"script\",\"style\",\"nav\",\"header\",\"footer\",\"aside\"]):\n",
    "        t.decompose()\n",
    "\n",
    "    text = soup.get_text(\"\\n\", strip=True)\n",
    "    title = soup.title.string.strip() if soup.title else None\n",
    "\n",
    "    return title, text\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# PROCESS ONE ARTICLE\n",
    "# ---------------------------------------------\n",
    "def process_article(row):\n",
    "    aid, url, status, html, ts = row\n",
    "\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    # primary extractor\n",
    "    title, text = extract_content(html)\n",
    "\n",
    "    # fallback\n",
    "    if not text or len(text) < 200:\n",
    "        title, text = fallback_extract(html)\n",
    "\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # source domain\n",
    "    domain = urlparse(url).netloc\n",
    "\n",
    "    # date extraction\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    published = extract_date(soup)\n",
    "\n",
    "    return {\n",
    "        \"id\": aid,\n",
    "        \"url\": url,\n",
    "        \"source\": domain,\n",
    "        \"title\": title,\n",
    "        \"published\": published,\n",
    "        \"text\": text,\n",
    "        \"html_len\": len(html)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------\n",
    "def run():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    init_clean_table(conn)\n",
    "\n",
    "    # fetch ALL raw html rows\n",
    "    raw_rows = conn.execute(\n",
    "        \"SELECT id, url, status, html, ts FROM articles WHERE status='done'\"\n",
    "    ).fetchall()\n",
    "\n",
    "    # find already cleaned IDs (resume)\n",
    "    cleaned_ids = {\n",
    "        r[0] for r in conn.execute(\n",
    "            \"SELECT id FROM clean_articles\"\n",
    "        ).fetchall()\n",
    "    }\n",
    "\n",
    "    print(f\"[info] Raw articles: {len(raw_rows)}\")\n",
    "    print(f\"[info] Already cleaned: {len(cleaned_ids)}\")\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for row in raw_rows:\n",
    "        aid = row[0]\n",
    "        if aid in cleaned_ids:\n",
    "            continue\n",
    "\n",
    "        result = process_article(row)\n",
    "\n",
    "        if result:\n",
    "            conn.execute(\"\"\"\n",
    "                INSERT OR REPLACE INTO clean_articles\n",
    "                (id, url, source, title, published, text, html_len, extracted_at)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                result[\"id\"],\n",
    "                result[\"url\"],\n",
    "                result[\"source\"],\n",
    "                result[\"title\"],\n",
    "                result[\"published\"],\n",
    "                result[\"text\"],\n",
    "                result[\"html_len\"],\n",
    "                int(time.time())\n",
    "            ))\n",
    "            conn.commit()\n",
    "\n",
    "            print(f\"[ok] Cleaned ID {aid} ({result['source']})\")\n",
    "        else:\n",
    "            print(f\"[skip] ID {aid}: could not extract text\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    print(f\"[DONE] Cleaned {count} articles.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1R_COXXx1qwMO9FJSwK4SELdmUh8D0BBp",
     "timestamp": 1763078636563
    },
    {
     "file_id": "/v2/external/notebooks/intro.ipynb",
     "timestamp": 1763063487961
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
